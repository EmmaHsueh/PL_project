{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoUuqjzL4PjYHP+dbW5/tS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmmaHsueh/PL_project/blob/main/Automated_Financial_News_Analysis_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q requests beautifulsoup4 pandas gradio google-generativeai gspread oauth2client scikit-learn jieba feedparser wordcloud matplotlib plotly kaleido"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei5PbjIsM0Iv",
        "outputId": "61875a3d-d44b-4da9-b6ab-bcfa3c1ba069"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/66.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==================== 匯入套件 ====================\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import gradio as gr\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from google.auth import default\n",
        "import google.generativeai as genai\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import jieba\n",
        "import jieba.analyse\n",
        "from collections import Counter\n",
        "import re\n",
        "import feedparser\n"
      ],
      "metadata": {
        "id": "lfqywNcCM1q7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==================== Google Sheets 設定 ====================\n",
        "def setup_google_sheets():\n",
        "    \"\"\"設定 Google Sheets 連線\"\"\"\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "    return gc\n"
      ],
      "metadata": {
        "id": "QU33teX7M23c"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==================== Gemini API 設定 ====================\n",
        "def setup_gemini(api_key):\n",
        "    \"\"\"設定 Gemini API\"\"\"\n",
        "    genai.configure(api_key=api_key)\n",
        "    model = genai.GenerativeModel('models/gemini-2.5-pro')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "up8sO6j6M4lG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==================== 財經新聞爬蟲（改良版）====================\n",
        "class FinanceNewsCrawler:\n",
        "    \"\"\"爬取台股財經新聞 - 使用多種穩定來源\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',\n",
        "        }\n",
        "        self.news_data = []\n",
        "\n",
        "    def crawl_udn_rss(self, max_news=30):\n",
        "        \"\"\"方法1: 爬取聯合新聞網 RSS（最穩定）\"\"\"\n",
        "        self.news_data = []\n",
        "\n",
        "        try:\n",
        "            print(\"🔍 正在爬取聯合新聞網 RSS...\")\n",
        "            rss_urls = [\n",
        "                'https://udn.com/rssfeed/news/2/6644?ch=news',  # 股市要聞\n",
        "                'https://udn.com/rssfeed/news/2/6645?ch=news',  # 上市電子\n",
        "            ]\n",
        "\n",
        "            for rss_url in rss_urls:\n",
        "                try:\n",
        "                    feed = feedparser.parse(rss_url)\n",
        "\n",
        "                    for entry in feed.entries[:15]:\n",
        "                        self.news_data.append({\n",
        "                            '標題': entry.title,\n",
        "                            '摘要': entry.summary if hasattr(entry, 'summary') else '',\n",
        "                            '連結': entry.link,\n",
        "                            '發布時間': entry.published if hasattr(entry, 'published') else '未知時間',\n",
        "                            '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                            '來源': '聯合新聞網'\n",
        "                        })\n",
        "\n",
        "                        if len(self.news_data) >= max_news:\n",
        "                            break\n",
        "\n",
        "                    time.sleep(1)\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ RSS 爬取部分失敗: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            print(f\"✅ 成功爬取 {len(self.news_data)} 則新聞（聯合新聞網）\")\n",
        "            return self.news_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ RSS 爬取失敗: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def crawl_ctee_news(self, max_news=30):\n",
        "        \"\"\"方法2: 爬取工商時報（備用方案）\"\"\"\n",
        "        self.news_data = []\n",
        "\n",
        "        try:\n",
        "            print(\"🔍 正在爬取工商時報...\")\n",
        "            url = \"https://ctee.com.tw/news/stock\"\n",
        "\n",
        "            response = requests.get(url, headers=self.headers, timeout=15)\n",
        "            response.encoding = 'utf-8'\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # 找到新聞列表\n",
        "            news_items = soup.find_all('div', class_='item')\n",
        "\n",
        "            for item in news_items[:max_news]:\n",
        "                try:\n",
        "                    title_tag = item.find('h3')\n",
        "                    link_tag = item.find('a')\n",
        "                    time_tag = item.find('time')\n",
        "\n",
        "                    if title_tag and link_tag:\n",
        "                        title = title_tag.get_text(strip=True)\n",
        "                        link = link_tag['href'] if link_tag['href'].startswith('http') else 'https://ctee.com.tw' + link_tag['href']\n",
        "                        publish_time = time_tag.get_text(strip=True) if time_tag else '未知時間'\n",
        "\n",
        "                        self.news_data.append({\n",
        "                            '標題': title,\n",
        "                            '摘要': '',\n",
        "                            '連結': link,\n",
        "                            '發布時間': publish_time,\n",
        "                            '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                            '來源': '工商時報'\n",
        "                        })\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "            print(f\"✅ 成功爬取 {len(self.news_data)} 則新聞（工商時報）\")\n",
        "            return self.news_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ 工商時報爬取失敗: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def crawl_demo_data(self):\n",
        "        \"\"\"方法3: 使用示範資料（保證可執行）\"\"\"\n",
        "        print(\"🔍 使用示範新聞資料...\")\n",
        "\n",
        "        self.news_data = [\n",
        "            {'標題': '台積電法說會釋利多 外資目標價上看1200元', '摘要': '台積電舉行法說會，釋出多項利多消息，外資看好後市', '連結': 'https://example.com/1', '發布時間': '2025-10-26 10:30', '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '來源': '示範資料'},\n",
        "            {'標題': '美股科技股大漲 台股電子股可望受惠', '摘要': '美國科技股強勁上漲，帶動台灣電子股買氣', '連結': 'https://example.com/2', '發布時間': '2025-10-26 09:15', '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '來源': '示範資料'},\n",
        "            {'標題': 'AI需求強勁 半導體產業鏈營收創新高', '摘要': '人工智慧應用帶動半導體需求大增', '連結': 'https://example.com/3', '發布時間': '2025-10-26 08:45', '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '來源': '示範資料'},\n",
        "            {'標題': '台股站穩萬八 外資連續買超', '摘要': '台灣股市表現強勁，外資持續買入', '連結': 'https://example.com/4', '發布時間': '2025-10-25 16:20', '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '來源': '示範資料'},\n",
        "            {'標題': '聯發科推出新款5G晶片 獲市場好評', '摘要': '聯發科最新晶片效能提升，預期營收成長', '連結': 'https://example.com/5', '發布時間': '2025-10-25 14:30', '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '來源': '示範資料'},\n",
        "            {'標題': '鴻海電動車佈局加速 訂單能見度提升', '摘要': '鴻海在電動車領域持續擴展，獲得多筆訂單', '連結': 'https://example.com/6', '發布時間': '2025-10-25 11:00', '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '來源': '示範資料'},\n",
        "            {'標題': '金融股配息題材發酵 吸引存股族', '摘要': '金融股進入除權息旺季，殖利率吸引人', '連結': 'https://example.com/7', '發布時間': '2025-10-24 15:45', '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '來源': '示範資料'},\n",
        "            {'標題': '台股基金單週吸金破百億 投資人搶進', '摘要': '台股基金受到投資人青睞，資金大量湧入', '連結': 'https://example.com/8', '發布時間': '2025-10-24 13:20', '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '來源': '示範資料'},\n",
        "            {'標題': '航運股受油價影響 股價震盪', '摘要': '國際油價波動，影響航運業成本', '連結': 'https://example.com/9', '發布時間': '2025-10-24 10:10', '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '來源': '示範資料'},\n",
        "            {'標題': '生技股獲政府補助 題材發酵', '摘要': '政府加碼生技產業補助，相關個股受惠', '連結': 'https://example.com/10', '發布時間': '2025-10-23 16:50', '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '來源': '示範資料'},\n",
        "            {'標題': '台幣升值壓力 出口產業受關注', '摘要': '新台幣匯率走強，出口企業面臨挑戰', '連結': 'https://example.com/11', '發布時間': '2025-10-23 14:25', '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '來源': '示範資料'},\n",
        "            {'標題': '電動車供應鏈夯 相關個股漲勢凌厲', '摘要': '電動車產業鏈持續成長，帶動股價', '連結': 'https://example.com/12', '發布時間': '2025-10-23 11:30', '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '來源': '示範資料'},\n",
        "            {'標題': '記憶體價格回升 相關廠商營收看增', '摘要': 'DRAM與NAND價格上漲，記憶體廠受惠', '連結': 'https://example.com/13', '發布時間': '2025-10-22 15:15', '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '來源': '示範資料'},\n",
        "            {'標題': '被動元件缺貨 廠商調漲報價', '摘要': '被動元件供不應求，價格持續上漲', '連結': 'https://example.com/14', '發布時間': '2025-10-22 12:40', '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '來源': '示範資料'},\n",
        "            {'標題': '5G基建投資加速 電信股受矚目', '摘要': '各國加速5G建設，電信業迎來商機', '連結': 'https://example.com/15', '發布時間': '2025-10-22 09:55', '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '來源': '示範資料'},\n",
        "            {'標題': '綠能政策推動 太陽能概念股發光', '摘要': '政府綠能政策明確，太陽能產業受惠', '連結': 'https://example.com/16', '發布時間': '2025-10-21 16:30', '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '來源': '示範資料'},\n",
        "            {'標題': '晶圓代工產能滿載 業者擴廠加速', '摘要': '晶圓代工需求旺盛，廠商積極擴產', '連結': 'https://example.com/17', '發布時間': '2025-10-21 13:45', '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '來源': '示範資料'},\n",
        "            {'標題': 'PCB產業需求回溫 法人喊買', '摘要': '印刷電路板需求復甦，法人看好後市', '連結': 'https://example.com/18', '發布時間': '2025-10-21 10:20', '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '來源': '示範資料'},\n",
        "            {'標題': '台股權值股輪動 中小型股活躍', '摘要': '大盤輪動效應明顯，中小型股交易熱絡', '連結': 'https://example.com/19', '發布時間': '2025-10-20 15:10', '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '來源': '示範資料'},\n",
        "            {'標題': '科技業人才荒 各公司加薪搶人', '摘要': '科技產業缺工嚴重，企業提高薪資', '連結': 'https://example.com/20', '發布時間': '2025-10-20 11:55', '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), '來源': '示範資料'},\n",
        "        ]\n",
        "\n",
        "        print(f\"✅ 載入 {len(self.news_data)} 則示範新聞\")\n",
        "        return self.news_data\n",
        "\n",
        "    def auto_crawl(self, max_news=30):\n",
        "        \"\"\"自動選擇最佳爬蟲方法\"\"\"\n",
        "        print(\"🚀 開始智慧爬蟲...\")\n",
        "\n",
        "        # 優先順序：RSS > 工商時報 > 示範資料\n",
        "        methods = [\n",
        "            ('RSS Feed', self.crawl_udn_rss),\n",
        "            ('工商時報', self.crawl_ctee_news),\n",
        "            ('示範資料', self.crawl_demo_data)\n",
        "        ]\n",
        "\n",
        "        for method_name, method in methods:\n",
        "            try:\n",
        "                print(f\"📡 嘗試使用: {method_name}\")\n",
        "                if method_name == '示範資料':\n",
        "                    news = method()\n",
        "                else:\n",
        "                    news = method(max_news)\n",
        "\n",
        "                if news and len(news) > 0:\n",
        "                    print(f\"✅ 成功！使用 {method_name} 取得 {len(news)} 則新聞\")\n",
        "                    return news\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ {method_name} 失敗，嘗試下一個方法...\")\n",
        "                continue\n",
        "\n",
        "        print(\"❌ 所有方法都失敗\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "Y8VI3TKBQzuo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==================== 財經新聞爬蟲 ====================\n",
        "class FinanceNewsCrawler:\n",
        "    \"\"\"爬取台股財經新聞\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "        self.news_data = []\n",
        "\n",
        "    def crawl_cnyes_news(self, pages=3):\n",
        "        \"\"\"爬取鉅亨網台股新聞\"\"\"\n",
        "        self.news_data = []\n",
        "        base_url = \"https://news.cnyes.com/news/cat/tw_stock\"\n",
        "\n",
        "        for page in range(1, pages + 1):\n",
        "            try:\n",
        "                print(f\"🔍 正在爬取第 {page}/{pages} 頁...\")\n",
        "                url = f\"{base_url}?page={page}\"\n",
        "                response = requests.get(url, headers=self.headers, timeout=10)\n",
        "                response.encoding = 'utf-8'\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                # 找到新聞列表\n",
        "                news_items = soup.find_all('div', class_='_1Zdp')\n",
        "\n",
        "                for item in news_items:\n",
        "                    try:\n",
        "                        # 標題\n",
        "                        title_tag = item.find('a')\n",
        "                        if not title_tag:\n",
        "                            continue\n",
        "\n",
        "                        title = title_tag.get_text(strip=True)\n",
        "                        link = \"https://news.cnyes.com\" + title_tag['href']\n",
        "\n",
        "                        # 時間\n",
        "                        time_tag = item.find('time')\n",
        "                        publish_time = time_tag.get_text(strip=True) if time_tag else \"未知時間\"\n",
        "\n",
        "                        # 摘要\n",
        "                        summary_tag = item.find('p')\n",
        "                        summary = summary_tag.get_text(strip=True) if summary_tag else \"\"\n",
        "\n",
        "                        self.news_data.append({\n",
        "                            '標題': title,\n",
        "                            '摘要': summary,\n",
        "                            '連結': link,\n",
        "                            '發布時間': publish_time,\n",
        "                            '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                        })\n",
        "                    except Exception as e:\n",
        "                        continue\n",
        "\n",
        "                time.sleep(2)  # 避免請求太快\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ 第 {page} 頁爬取失敗: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"✅ 成功爬取 {len(self.news_data)} 則新聞\")\n",
        "        return self.news_data\n",
        "\n",
        "    def crawl_yahoo_finance_news(self, pages=3):\n",
        "        \"\"\"爬取 Yahoo 財經台股新聞\"\"\"\n",
        "        self.news_data = []\n",
        "\n",
        "        for page in range(pages):\n",
        "            try:\n",
        "                print(f\"🔍 正在爬取第 {page+1}/{pages} 頁...\")\n",
        "                url = f\"https://tw.stock.yahoo.com/news/list?category=%E5%8F%B0%E8%82%A1&offset={page*10}\"\n",
        "                response = requests.get(url, headers=self.headers, timeout=10)\n",
        "                response.encoding = 'utf-8'\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                # 找到新聞列表\n",
        "                news_items = soup.find_all('div', class_='Ov(h)')\n",
        "\n",
        "                for item in news_items:\n",
        "                    try:\n",
        "                        title_tag = item.find('a')\n",
        "                        if not title_tag:\n",
        "                            continue\n",
        "\n",
        "                        title = title_tag.get_text(strip=True)\n",
        "                        link = \"https://tw.stock.yahoo.com\" + title_tag['href']\n",
        "\n",
        "                        # 時間\n",
        "                        time_tag = item.find('time')\n",
        "                        publish_time = time_tag.get_text(strip=True) if time_tag else \"未知時間\"\n",
        "\n",
        "                        self.news_data.append({\n",
        "                            '標題': title,\n",
        "                            '摘要': '',\n",
        "                            '連結': link,\n",
        "                            '發布時間': publish_time,\n",
        "                            '爬取時間': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                        })\n",
        "                    except Exception as e:\n",
        "                        continue\n",
        "\n",
        "                time.sleep(2)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ 第 {page+1} 頁爬取失敗: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"✅ 成功爬取 {len(self.news_data)} 則新聞\")\n",
        "        return self.news_data"
      ],
      "metadata": {
        "id": "o3sKVIGVM8-m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 文字分析（完全修正版）====================\n",
        "class TextAnalyzer:\n",
        "    \"\"\"文字分析與關鍵字提取 - 結巴分詞增強版\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"🔧 初始化結巴分詞...\")\n",
        "\n",
        "        # 載入結巴分詞\n",
        "        import jieba\n",
        "        import jieba.analyse\n",
        "\n",
        "        # 擴充停用詞列表（包含網址相關詞）\n",
        "        self.stopwords = set([\n",
        "            # 基本停用詞\n",
        "            '的', '是', '在', '了', '和', '與', '等', '將', '可', '或',\n",
        "            '但', '對', '為', '及', '以', '於', '從', '更', '很', '最',\n",
        "            '也', '都', '就', '到', '被', '有', '這', '那', '個', '他',\n",
        "            '她', '我', '你', '們', '中', '上', '下', '來', '去', '說',\n",
        "            '要', '會', '能', '把', '讓', '給', '沒', '只', '還', '又',\n",
        "            '則', '篇', '位', '家', '間', '名', '次', '項', '件', '張',\n",
        "            '不', '其', '而', '因', '所', '則', '得', '著', '過', '便',\n",
        "            '用', '並', '向', '如', '且', '再', '另', '即', '或許', '此',\n",
        "            '表示', '指出', '認為', '根據', '透過', '目前', '今年', '去年',\n",
        "            # 網址相關停用詞（關鍵！）\n",
        "            'http', 'https', 'www', 'com', 'tw', 'net', 'org', 'url',\n",
        "            'amp', 'photo', 'image', 'jpg', 'png', 'html', 'php',\n",
        "            'udn', 'news', 'article', 'id', 'link', 'href',\n",
        "            # 數字相關\n",
        "            '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "            '10', '20', '30', '100', '180', '360', '720', '1080',\n",
        "            '3600', '2024', '2025', '2026',\n",
        "            # 時間相關通用詞\n",
        "            '今天', '昨天', '明天', '上午', '下午', '晚上'\n",
        "        ])\n",
        "\n",
        "        # 自訂詞典（金融相關詞彙）\n",
        "        financial_terms = [\n",
        "            # 公司名稱\n",
        "            '台積電', '聯發科', '鴻海', '大立光', '台達電', '聯電',\n",
        "            '日月光', '國巨', '華碩', '廣達', '仁寶', '和碩',\n",
        "            '台塑', '中鋼', '富邦金', '國泰金', '兆豐金', '玉山金',\n",
        "            # 金融術語\n",
        "            '外資', '投信', '自營商', '三大法人', '散戶',\n",
        "            '漲停', '跌停', '漲跌幅', '成交量', '融資', '融券',\n",
        "            '本益比', '殖利率', '股息', '配息', '除權息',\n",
        "            '現金股利', '股票股利', '填權息',\n",
        "            # 產業類別\n",
        "            '半導體', '電子股', '金融股', '傳產股', '生技股',\n",
        "            '航運股', '鋼鐵股', '塑化股', '營建股', '觀光股',\n",
        "            # 財務指標\n",
        "            '營收', '獲利', '財報', 'EPS', 'ROE', 'ROA',\n",
        "            '毛利率', '營益率', '淨利率', '負債比',\n",
        "            # 市場相關\n",
        "            '美股', '台股', '陸股', '港股', '日股',\n",
        "            '加權指數', '櫃買指數', '道瓊', '那斯達克', 'S&P',\n",
        "            '多頭', '空頭', '盤整', '突破', '支撐', '壓力',\n",
        "            # 總經相關\n",
        "            '升息', '降息', '通膨', '通縮', '經濟成長', 'GDP',\n",
        "            '央行', '聯準會', '貨幣政策', '財政政策',\n",
        "            # 科技趨勢\n",
        "            'AI', '人工智慧', '電動車', '5G', '物聯網',\n",
        "            '元宇宙', '區塊鏈', '雲端', '大數據',\n",
        "            # 其他\n",
        "            '投資', '買進', '賣出', '持有', '布局', '獲利了結'\n",
        "        ]\n",
        "\n",
        "        # 將自訂詞加入結巴詞典\n",
        "        for term in financial_terms:\n",
        "            jieba.add_word(term, freq=10000)  # 設定高頻率確保被識別\n",
        "\n",
        "        print(\"✅ 結巴分詞初始化完成\")\n",
        "        print(f\"📚 停用詞數量: {len(self.stopwords)} 個\")\n",
        "        print(f\"📖 自訂詞彙數量: {len(financial_terms)} 個\")\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"文字預處理 - 清除網址和雜訊\"\"\"\n",
        "        # 1. 移除網址\n",
        "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "        # 2. 移除 email\n",
        "        text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "        # 3. 移除數字（但保留中文數字）\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "        # 4. 移除特殊符號（保留中文、英文）\n",
        "        text = re.sub(r'[^\\w\\s\\u4e00-\\u9fff]', ' ', text)\n",
        "\n",
        "        # 5. 移除多餘空白\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        return text\n",
        "\n",
        "    def is_valid_keyword(self, word):\n",
        "        \"\"\"判斷是否為有效關鍵字\"\"\"\n",
        "        # 檢查長度\n",
        "        if len(word) < 2:\n",
        "            return False\n",
        "\n",
        "        # 檢查是否為停用詞\n",
        "        if word.lower() in self.stopwords:\n",
        "            return False\n",
        "\n",
        "        # 檢查是否全為數字\n",
        "        if word.isdigit():\n",
        "            return False\n",
        "\n",
        "        # 檢查是否為純英文且太短\n",
        "        if word.isalpha() and len(word) < 3:\n",
        "            return False\n",
        "\n",
        "        # 檢查是否包含網址關鍵字\n",
        "        url_keywords = ['http', 'www', 'com', 'tw', 'net', 'html', 'php', 'amp']\n",
        "        if any(uk in word.lower() for uk in url_keywords):\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def extract_keywords_jieba(self, texts, top_n=10):\n",
        "        \"\"\"使用 jieba.analyse 提取關鍵字\"\"\"\n",
        "        print(\"🔍 使用 jieba.analyse 提取關鍵字...\")\n",
        "\n",
        "        # 預處理並合併所有文字\n",
        "        processed_texts = [self.preprocess_text(text) for text in texts]\n",
        "        combined_text = ' '.join(processed_texts)\n",
        "\n",
        "        # 使用 jieba 的 TF-IDF 提取關鍵字\n",
        "        keywords = jieba.analyse.extract_tags(\n",
        "            combined_text,\n",
        "            topK=top_n * 3,  # 提取更多，之後再過濾\n",
        "            withWeight=True,\n",
        "            allowPOS=('n', 'nr', 'ns', 'nt', 'nz', 'v', 'vn', 'a', 'an')\n",
        "        )\n",
        "\n",
        "        # 過濾無效關鍵字\n",
        "        valid_keywords = [\n",
        "            (word, weight) for word, weight in keywords\n",
        "            if self.is_valid_keyword(word)\n",
        "        ]\n",
        "\n",
        "        print(f\"✅ 提取了 {len(valid_keywords[:top_n])} 個有效關鍵字\")\n",
        "        return valid_keywords[:top_n]\n",
        "\n",
        "    def extract_keywords_textrank(self, texts, top_n=10):\n",
        "        \"\"\"使用 TextRank 演算法提取關鍵字\"\"\"\n",
        "        print(\"🔍 使用 TextRank 演算法提取關鍵字...\")\n",
        "\n",
        "        # 預處理並合併所有文字\n",
        "        processed_texts = [self.preprocess_text(text) for text in texts]\n",
        "        combined_text = ' '.join(processed_texts)\n",
        "\n",
        "        # 使用 jieba 的 TextRank 提取關鍵字\n",
        "        keywords = jieba.analyse.textrank(\n",
        "            combined_text,\n",
        "            topK=top_n * 3,\n",
        "            withWeight=True,\n",
        "            allowPOS=('n', 'nr', 'ns', 'nt', 'nz', 'v', 'vn', 'a', 'an')\n",
        "        )\n",
        "\n",
        "        # 過濾無效關鍵字\n",
        "        valid_keywords = [\n",
        "            (word, weight) for word, weight in keywords\n",
        "            if self.is_valid_keyword(word)\n",
        "        ]\n",
        "\n",
        "        print(f\"✅ 提取了 {len(valid_keywords[:top_n])} 個有效關鍵字\")\n",
        "        return valid_keywords[:top_n]\n",
        "\n",
        "    def word_frequency(self, texts, top_n=10):\n",
        "        \"\"\"詞頻統計\"\"\"\n",
        "        print(\"🔍 進行詞頻統計...\")\n",
        "\n",
        "        # 預處理並合併所有文字\n",
        "        processed_texts = [self.preprocess_text(text) for text in texts]\n",
        "        combined_text = ' '.join(processed_texts)\n",
        "\n",
        "        # 使用結巴分詞\n",
        "        words = jieba.cut(combined_text)\n",
        "\n",
        "        # 過濾並統計\n",
        "        valid_words = [\n",
        "            w.strip() for w in words\n",
        "            if w.strip() and self.is_valid_keyword(w.strip())\n",
        "        ]\n",
        "\n",
        "        # 計算詞頻\n",
        "        word_counts = Counter(valid_words)\n",
        "\n",
        "        result = word_counts.most_common(top_n)\n",
        "        print(f\"✅ 統計了 {len(word_counts)} 個不同的詞，返回前 {top_n} 個\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def tfidf_analysis(self, texts, top_n=10):\n",
        "        \"\"\"使用 sklearn TF-IDF 分析\"\"\"\n",
        "        print(\"🔍 使用 TF-IDF 分析關鍵字...\")\n",
        "\n",
        "        # 中文分詞函數\n",
        "        def tokenize(text):\n",
        "            text = self.preprocess_text(text)\n",
        "            words = jieba.cut(text)\n",
        "            valid = [w.strip() for w in words if w.strip() and self.is_valid_keyword(w.strip())]\n",
        "            return ' '.join(valid)\n",
        "\n",
        "        # 對所有文字進行分詞\n",
        "        tokenized_texts = [tokenize(text) for text in texts]\n",
        "\n",
        "        # 過濾空文本\n",
        "        tokenized_texts = [t for t in tokenized_texts if t.strip()]\n",
        "\n",
        "        if not tokenized_texts:\n",
        "            print(\"⚠️ 沒有有效文本，改用 jieba 方法\")\n",
        "            return self.extract_keywords_jieba(texts, top_n)\n",
        "\n",
        "        try:\n",
        "            # 建立 TF-IDF 向量化器\n",
        "            vectorizer = TfidfVectorizer(\n",
        "                max_features=top_n * 3,\n",
        "                token_pattern=r'(?u)\\b\\w\\w+\\b',  # 至少2個字符\n",
        "                min_df=1,  # 至少出現在1個文檔中\n",
        "                max_df=0.8  # 最多出現在80%的文檔中\n",
        "            )\n",
        "\n",
        "            # 計算 TF-IDF 矩陣\n",
        "            tfidf_matrix = vectorizer.fit_transform(tokenized_texts)\n",
        "\n",
        "            # 取得特徵名稱\n",
        "            feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "            # 計算每個關鍵字的總分數\n",
        "            tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
        "\n",
        "            # 建立配對\n",
        "            keywords = list(zip(feature_names, tfidf_scores))\n",
        "\n",
        "            # 再次過濾（雙重保險）\n",
        "            keywords = [(w, s) for w, s in keywords if self.is_valid_keyword(w)]\n",
        "\n",
        "            # 排序\n",
        "            keywords.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            print(f\"✅ TF-IDF 分析完成，提取 {min(top_n, len(keywords))} 個關鍵字\")\n",
        "\n",
        "            return keywords[:top_n]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ TF-IDF 分析失敗: {str(e)}\")\n",
        "            print(\"🔄 改用 jieba 關鍵字提取...\")\n",
        "            return self.extract_keywords_jieba(texts, top_n)\n",
        "\n",
        "    def comprehensive_analysis(self, texts, top_n=10):\n",
        "        \"\"\"綜合分析\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"🚀 開始綜合文字分析（已過濾網址和雜訊）\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        results = {\n",
        "            'word_frequency': [],\n",
        "            'jieba_tfidf': [],\n",
        "            'textrank': [],\n",
        "            'sklearn_tfidf': []\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            print(\"\\n📊 方法1: 詞頻統計\")\n",
        "            results['word_frequency'] = self.word_frequency(texts, top_n)\n",
        "            self._print_keywords(results['word_frequency'], '詞頻統計')\n",
        "        except Exception as e:\n",
        "            print(f\"❌ 詞頻統計失敗: {str(e)}\")\n",
        "\n",
        "        try:\n",
        "            print(\"\\n📊 方法2: jieba TF-IDF\")\n",
        "            results['jieba_tfidf'] = self.extract_keywords_jieba(texts, top_n)\n",
        "            self._print_keywords(results['jieba_tfidf'], 'jieba TF-IDF')\n",
        "        except Exception as e:\n",
        "            print(f\"❌ jieba TF-IDF 失敗: {str(e)}\")\n",
        "\n",
        "        try:\n",
        "            print(\"\\n📊 方法3: TextRank 演算法\")\n",
        "            results['textrank'] = self.extract_keywords_textrank(texts, top_n)\n",
        "            self._print_keywords(results['textrank'], 'TextRank')\n",
        "        except Exception as e:\n",
        "            print(f\"❌ TextRank 失敗: {str(e)}\")\n",
        "\n",
        "        try:\n",
        "            print(\"\\n📊 方法4: sklearn TF-IDF\")\n",
        "            results['sklearn_tfidf'] = self.tfidf_analysis(texts, top_n)\n",
        "            self._print_keywords(results['sklearn_tfidf'], 'sklearn TF-IDF')\n",
        "        except Exception as e:\n",
        "            print(f\"❌ sklearn TF-IDF 失敗: {str(e)}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"✅ 綜合分析完成\")\n",
        "        print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _print_keywords(self, keywords, method_name):\n",
        "        \"\"\"輔助函數：印出關鍵字\"\"\"\n",
        "        if keywords:\n",
        "            print(f\"\\n【{method_name}】前 10 個關鍵字:\")\n",
        "            for idx, (word, score) in enumerate(keywords[:10], 1):\n",
        "                print(f\"  {idx}. {word} ({score:.4f})\")\n",
        "        else:\n",
        "            print(f\"【{method_name}】無結果\")\n",
        "\n",
        "    def get_best_keywords(self, texts, top_n=10, method='tfidf'):\n",
        "        \"\"\"取得最佳關鍵字\"\"\"\n",
        "        if method == 'tfidf':\n",
        "            return self.tfidf_analysis(texts, top_n)\n",
        "        elif method == 'textrank':\n",
        "            return self.extract_keywords_textrank(texts, top_n)\n",
        "        elif method == 'frequency':\n",
        "            return self.word_frequency(texts, top_n)\n",
        "        elif method == 'comprehensive':\n",
        "            # 綜合多種方法\n",
        "            jieba_kw = dict(self.extract_keywords_jieba(texts, top_n * 2))\n",
        "            textrank_kw = dict(self.extract_keywords_textrank(texts, top_n * 2))\n",
        "\n",
        "            all_keywords = set(jieba_kw.keys()) | set(textrank_kw.keys())\n",
        "            combined_scores = {}\n",
        "\n",
        "            for kw in all_keywords:\n",
        "                if self.is_valid_keyword(kw):  # 再次確認\n",
        "                    score = 0\n",
        "                    if kw in jieba_kw:\n",
        "                        score += jieba_kw[kw]\n",
        "                    if kw in textrank_kw:\n",
        "                        score += textrank_kw[kw]\n",
        "                    combined_scores[kw] = score\n",
        "\n",
        "            sorted_kw = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "            return sorted_kw[:top_n]\n",
        "        else:\n",
        "            return self.tfidf_analysis(texts, top_n)"
      ],
      "metadata": {
        "id": "8u1zPfSRNBdB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 修正後的視覺化模組 ====================\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "import base64\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class DataVisualizer:\n",
        "    \"\"\"資料視覺化類別 - 使用 Plotly\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.colors = px.colors.qualitative.Set3\n",
        "\n",
        "    def create_wordcloud(self, keywords, title=\"關鍵字文字雲\", max_words=50):\n",
        "        \"\"\"\n",
        "        生成文字雲圖 - 修正版（返回PIL Image）\n",
        "        keywords: [(詞, 分數), ...] 格式\n",
        "        \"\"\"\n",
        "        print(f\"🎨 生成文字雲: {title}\")\n",
        "\n",
        "        try:\n",
        "            # 準備文字雲資料\n",
        "            word_freq = {word: float(score) for word, score in keywords[:max_words]}\n",
        "\n",
        "            if not word_freq:\n",
        "                print(\"⚠️ 沒有關鍵字資料\")\n",
        "                return None\n",
        "\n",
        "            # 生成文字雲\n",
        "            wordcloud = WordCloud(\n",
        "                width=1600,\n",
        "                height=800,\n",
        "                background_color='white',\n",
        "                font_path='/usr/share/fonts/truetype/wqy/wqy-microhei.ttc',  # 中文字體\n",
        "                colormap='viridis',\n",
        "                max_words=max_words,\n",
        "                relative_scaling=0.5,\n",
        "                min_font_size=12,\n",
        "                prefer_horizontal=0.7,\n",
        "                collocations=False  # 避免重複詞組\n",
        "            ).generate_from_frequencies(word_freq)\n",
        "\n",
        "            # 轉換為 numpy array\n",
        "            wordcloud_array = wordcloud.to_array()\n",
        "\n",
        "            # 轉換為 PIL Image\n",
        "            wordcloud_image = Image.fromarray(wordcloud_array)\n",
        "\n",
        "            # 儲存為臨時檔案\n",
        "            temp_path = '/tmp/wordcloud.png'\n",
        "            wordcloud_image.save(temp_path, format='PNG', dpi=(300, 300))\n",
        "\n",
        "            print(f\"✅ 文字雲生成完成 ({len(word_freq)} 個詞)\")\n",
        "            print(f\"📁 儲存路徑: {temp_path}\")\n",
        "\n",
        "            return temp_path  # 返回檔案路徑\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ 文字雲生成失敗: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def create_wordcloud_plotly(self, keywords, title=\"關鍵字文字雲\", max_words=50):\n",
        "        \"\"\"\n",
        "        使用 Plotly 製作文字雲替代方案（散點圖模擬）\n",
        "        \"\"\"\n",
        "        print(f\"🎨 生成 Plotly 文字雲: {title}\")\n",
        "\n",
        "        try:\n",
        "            top_keywords = keywords[:max_words]\n",
        "\n",
        "            if not top_keywords:\n",
        "                print(\"⚠️ 沒有關鍵字資料\")\n",
        "                return None\n",
        "\n",
        "            words = [kw for kw, _ in top_keywords]\n",
        "            scores = [float(score) for _, score in top_keywords]\n",
        "\n",
        "            # 正規化分數\n",
        "            max_score = max(scores)\n",
        "            min_score = min(scores)\n",
        "\n",
        "            # 計算字體大小（10-60）\n",
        "            sizes = [10 + (s - min_score) / (max_score - min_score) * 50 for s in scores]\n",
        "\n",
        "            # 隨機但固定的位置\n",
        "            import random\n",
        "            random.seed(42)\n",
        "\n",
        "            # 使用螺旋佈局\n",
        "            angles = [i * 137.5 for i in range(len(words))]  # 黃金角度\n",
        "            radii = [i ** 0.5 for i in range(len(words))]\n",
        "\n",
        "            x_coords = [r * np.cos(np.radians(a)) for r, a in zip(radii, angles)]\n",
        "            y_coords = [r * np.sin(np.radians(a)) for r, a in zip(radii, angles)]\n",
        "\n",
        "            # 建立散點圖\n",
        "            fig = go.Figure()\n",
        "\n",
        "            for i, (word, x, y, size, score) in enumerate(zip(words, x_coords, y_coords, sizes, scores)):\n",
        "                fig.add_trace(go.Scatter(\n",
        "                    x=[x],\n",
        "                    y=[y],\n",
        "                    mode='text',\n",
        "                    text=word,\n",
        "                    textfont=dict(\n",
        "                        size=size,\n",
        "                        color=f'hsl({i * 360 / len(words)}, 70%, 50%)'\n",
        "                    ),\n",
        "                    hovertemplate=f'<b>{word}</b><br>分數: {score:.4f}<extra></extra>',\n",
        "                    showlegend=False\n",
        "                ))\n",
        "\n",
        "            fig.update_layout(\n",
        "                title=dict(\n",
        "                    text=f'<b>{title}</b>',\n",
        "                    x=0.5,\n",
        "                    xanchor='center',\n",
        "                    font=dict(size=24)\n",
        "                ),\n",
        "                xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),\n",
        "                yaxis=dict(showgrid=False, showticklabels=False, zeroline=False),\n",
        "                height=800,\n",
        "                template='plotly_white',\n",
        "                hovermode='closest',\n",
        "                plot_bgcolor='rgba(240, 240, 240, 0.5)'\n",
        "            )\n",
        "\n",
        "            print(f\"✅ Plotly 文字雲生成完成\")\n",
        "            return fig\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Plotly 文字雲生成失敗: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def plot_top_keywords_bar(self, keywords, title=\"前20關鍵字排行\", top_n=20):\n",
        "        \"\"\"關鍵字橫條圖（Plotly）\"\"\"\n",
        "        print(f\"📊 生成關鍵字橫條圖: {title}\")\n",
        "\n",
        "        try:\n",
        "            top_keywords = keywords[:top_n]\n",
        "\n",
        "            if not top_keywords:\n",
        "                print(\"⚠️ 沒有關鍵字資料\")\n",
        "                return None\n",
        "\n",
        "            # 反轉順序\n",
        "            words = [kw for kw, _ in reversed(top_keywords)]\n",
        "            scores = [float(score) for _, score in reversed(top_keywords)]\n",
        "\n",
        "            # 建立橫條圖\n",
        "            fig = go.Figure(go.Bar(\n",
        "                x=scores,\n",
        "                y=words,\n",
        "                orientation='h',\n",
        "                marker=dict(\n",
        "                    color=scores,\n",
        "                    colorscale='Viridis',\n",
        "                    showscale=True,\n",
        "                    colorbar=dict(title=\"分數\")\n",
        "                ),\n",
        "                text=[f'{s:.4f}' for s in scores],\n",
        "                textposition='outside',\n",
        "                hovertemplate='<b>%{y}</b><br>分數: %{x:.4f}<extra></extra>'\n",
        "            ))\n",
        "\n",
        "            fig.update_layout(\n",
        "                title=dict(\n",
        "                    text=f'<b>{title}</b>',\n",
        "                    x=0.5,\n",
        "                    xanchor='center',\n",
        "                    font=dict(size=20)\n",
        "                ),\n",
        "                xaxis_title='TF-IDF 分數',\n",
        "                yaxis_title='關鍵字',\n",
        "                height=max(500, top_n * 30),\n",
        "                showlegend=False,\n",
        "                template='plotly_white',\n",
        "                margin=dict(l=120, r=50, t=80, b=50)\n",
        "            )\n",
        "\n",
        "            print(f\"✅ 橫條圖生成完成 ({len(top_keywords)} 個關鍵字)\")\n",
        "            return fig\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ 橫條圖生成失敗: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    # ... 其他視覺化方法保持不變 ...\n",
        "\n",
        "    def plot_keyword_distribution_pie(self, keywords, title=\"關鍵字分數分布\", top_n=15):\n",
        "        \"\"\"關鍵字圓餅圖\"\"\"\n",
        "        print(f\"📊 生成關鍵字圓餅圖: {title}\")\n",
        "\n",
        "        try:\n",
        "            top_keywords = keywords[:top_n]\n",
        "\n",
        "            if not top_keywords:\n",
        "                print(\"⚠️ 沒有關鍵字資料\")\n",
        "                return None\n",
        "\n",
        "            words = [kw for kw, _ in top_keywords]\n",
        "            scores = [float(score) for _, score in top_keywords]\n",
        "\n",
        "            fig = go.Figure(data=[go.Pie(\n",
        "                labels=words,\n",
        "                values=scores,\n",
        "                hole=0.4,\n",
        "                marker=dict(\n",
        "                    colors=px.colors.qualitative.Set3,\n",
        "                    line=dict(color='white', width=2)\n",
        "                ),\n",
        "                textinfo='label+percent',\n",
        "                hovertemplate='<b>%{label}</b><br>分數: %{value:.4f}<br>佔比: %{percent}<extra></extra>'\n",
        "            )])\n",
        "\n",
        "            fig.update_layout(\n",
        "                title=dict(\n",
        "                    text=f'<b>{title}</b>',\n",
        "                    x=0.5,\n",
        "                    xanchor='center',\n",
        "                    font=dict(size=20)\n",
        "                ),\n",
        "                showlegend=True,\n",
        "                height=600,\n",
        "                template='plotly_white',\n",
        "                legend=dict(\n",
        "                    orientation=\"v\",\n",
        "                    yanchor=\"middle\",\n",
        "                    y=0.5,\n",
        "                    xanchor=\"left\",\n",
        "                    x=1.05\n",
        "                )\n",
        "            )\n",
        "\n",
        "            print(f\"✅ 圓餅圖生成完成\")\n",
        "            return fig\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ 圓餅圖生成失敗: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def plot_methods_comparison(self, analysis_results, top_n=10):\n",
        "        \"\"\"比較不同分析方法\"\"\"\n",
        "        print(\"📊 生成分析方法比較圖\")\n",
        "\n",
        "        try:\n",
        "            methods = {\n",
        "                'word_frequency': '詞頻統計',\n",
        "                'jieba_tfidf': 'Jieba TF-IDF',\n",
        "                'textrank': 'TextRank',\n",
        "                'sklearn_tfidf': 'Sklearn TF-IDF'\n",
        "            }\n",
        "\n",
        "            all_keywords = set()\n",
        "            for method_key in methods.keys():\n",
        "                if method_key in analysis_results and analysis_results[method_key]:\n",
        "                    for word, _ in analysis_results[method_key][:top_n]:\n",
        "                        all_keywords.add(word)\n",
        "\n",
        "            if not all_keywords:\n",
        "                print(\"⚠️ 沒有關鍵字資料\")\n",
        "                return None\n",
        "\n",
        "            data = []\n",
        "            for method_key, method_name in methods.items():\n",
        "                if method_key in analysis_results and analysis_results[method_key]:\n",
        "                    keywords_dict = dict(analysis_results[method_key][:top_n])\n",
        "                    for word in all_keywords:\n",
        "                        score = float(keywords_dict.get(word, 0))\n",
        "                        if score > 0:\n",
        "                            data.append({\n",
        "                                '方法': method_name,\n",
        "                                '關鍵字': word,\n",
        "                                '分數': score\n",
        "                            })\n",
        "\n",
        "            if not data:\n",
        "                print(\"⚠️ 沒有有效資料\")\n",
        "                return None\n",
        "\n",
        "            df = pd.DataFrame(data)\n",
        "\n",
        "            fig = px.bar(\n",
        "                df,\n",
        "                x='分數',\n",
        "                y='關鍵字',\n",
        "                color='方法',\n",
        "                barmode='group',\n",
        "                title='<b>不同分析方法的關鍵字比較</b>',\n",
        "                labels={'分數': '分數', '關鍵字': '關鍵字', '方法': '分析方法'},\n",
        "                color_discrete_sequence=px.colors.qualitative.Set2,\n",
        "                height=max(600, len(all_keywords) * 40)\n",
        "            )\n",
        "\n",
        "            fig.update_layout(\n",
        "                template='plotly_white',\n",
        "                xaxis_title='分數',\n",
        "                yaxis_title='關鍵字',\n",
        "                legend=dict(\n",
        "                    orientation=\"h\",\n",
        "                    yanchor=\"bottom\",\n",
        "                    y=1.02,\n",
        "                    xanchor=\"right\",\n",
        "                    x=1\n",
        "                ),\n",
        "                margin=dict(l=120, r=50, t=100, b=50)\n",
        "            )\n",
        "\n",
        "            print(f\"✅ 方法比較圖生成完成\")\n",
        "            return fig\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ 方法比較圖生成失敗: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def plot_all_methods_heatmap(self, analysis_results, top_n=15):\n",
        "        \"\"\"熱力圖\"\"\"\n",
        "        print(\"📊 生成分析方法熱力圖\")\n",
        "\n",
        "        try:\n",
        "            methods = {\n",
        "                'word_frequency': '詞頻統計',\n",
        "                'jieba_tfidf': 'Jieba TF-IDF',\n",
        "                'textrank': 'TextRank',\n",
        "                'sklearn_tfidf': 'Sklearn TF-IDF'\n",
        "            }\n",
        "\n",
        "            all_keywords = set()\n",
        "            for method_key in methods.keys():\n",
        "                if method_key in analysis_results and analysis_results[method_key]:\n",
        "                    for word, _ in analysis_results[method_key][:top_n]:\n",
        "                        all_keywords.add(word)\n",
        "\n",
        "            if not all_keywords:\n",
        "                return None\n",
        "\n",
        "            keywords_list = sorted(list(all_keywords))\n",
        "            matrix = []\n",
        "            method_names = []\n",
        "\n",
        "            for method_key, method_name in methods.items():\n",
        "                if method_key in analysis_results and analysis_results[method_key]:\n",
        "                    method_names.append(method_name)\n",
        "                    keywords_dict = dict(analysis_results[method_key])\n",
        "                    row = [float(keywords_dict.get(kw, 0)) for kw in keywords_list]\n",
        "                    matrix.append(row)\n",
        "\n",
        "            if not matrix:\n",
        "                return None\n",
        "\n",
        "            fig = go.Figure(data=go.Heatmap(\n",
        "                z=matrix,\n",
        "                x=keywords_list,\n",
        "                y=method_names,\n",
        "                colorscale='Viridis',\n",
        "                hovertemplate='方法: %{y}<br>關鍵字: %{x}<br>分數: %{z:.4f}<extra></extra>',\n",
        "                colorbar=dict(title=\"分數\")\n",
        "            ))\n",
        "\n",
        "            fig.update_layout(\n",
        "                title='<b>關鍵字分析方法熱力圖</b>',\n",
        "                xaxis_title='關鍵字',\n",
        "                yaxis_title='分析方法',\n",
        "                height=400,\n",
        "                template='plotly_white',\n",
        "                xaxis=dict(tickangle=-45)\n",
        "            )\n",
        "\n",
        "            print(f\"✅ 熱力圖生成完成\")\n",
        "            return fig\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ 熱力圖生成失敗: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def plot_keyword_network(self, keywords, top_n=20):\n",
        "        \"\"\"關鍵字網絡圖\"\"\"\n",
        "        print(\"📊 生成關鍵字網絡圖\")\n",
        "\n",
        "        try:\n",
        "            top_keywords = keywords[:top_n]\n",
        "\n",
        "            if not top_keywords:\n",
        "                print(\"⚠️ 沒有關鍵字資料\")\n",
        "                return None\n",
        "\n",
        "            words = [kw for kw, _ in top_keywords]\n",
        "            scores = [float(score) for _, score in top_keywords]\n",
        "\n",
        "            max_score = max(scores)\n",
        "            sizes = [s / max_score * 100 + 20 for s in scores]\n",
        "\n",
        "            import random\n",
        "            random.seed(42)\n",
        "\n",
        "            x_coords = [random.uniform(0, 10) for _ in range(len(words))]\n",
        "            y_coords = [random.uniform(0, 10) for _ in range(len(words))]\n",
        "\n",
        "            fig = go.Figure(data=[go.Scatter(\n",
        "                x=x_coords,\n",
        "                y=y_coords,\n",
        "                mode='markers+text',\n",
        "                marker=dict(\n",
        "                    size=sizes,\n",
        "                    color=scores,\n",
        "                    colorscale='Viridis',\n",
        "                    showscale=True,\n",
        "                    colorbar=dict(title=\"TF-IDF<br>分數\"),\n",
        "                    line=dict(width=2, color='white')\n",
        "                ),\n",
        "                text=words,\n",
        "                textposition='middle center',\n",
        "                textfont=dict(size=10, color='white'),\n",
        "                hovertemplate='<b>%{text}</b><br>分數: %{marker.color:.4f}<extra></extra>'\n",
        "            )])\n",
        "\n",
        "            fig.update_layout(\n",
        "                title='<b>關鍵字網絡圖（泡泡大小代表重要性）</b>',\n",
        "                xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),\n",
        "                yaxis=dict(showgrid=False, showticklabels=False, zeroline=False),\n",
        "                height=700,\n",
        "                template='plotly_white',\n",
        "                showlegend=False\n",
        "            )\n",
        "\n",
        "            print(f\"✅ 網絡圖生成完成\")\n",
        "            return fig\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ 網絡圖生成失敗: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def plot_news_timeline(self, news_data):\n",
        "        \"\"\"新聞時間軸\"\"\"\n",
        "        print(\"📊 生成新聞時間軸\")\n",
        "\n",
        "        try:\n",
        "            if not news_data:\n",
        "                print(\"⚠️ 沒有新聞資料\")\n",
        "                return None\n",
        "\n",
        "            df = pd.DataFrame(news_data)\n",
        "            source_counts = df['來源'].value_counts()\n",
        "\n",
        "            fig = go.Figure(data=[\n",
        "                go.Bar(\n",
        "                    x=source_counts.index,\n",
        "                    y=source_counts.values,\n",
        "                    marker=dict(\n",
        "                        color=source_counts.values,\n",
        "                        colorscale='Blues',\n",
        "                        showscale=True,\n",
        "                        colorbar=dict(title=\"新聞數量\")\n",
        "                    ),\n",
        "                    text=source_counts.values,\n",
        "                    textposition='outside',\n",
        "                    hovertemplate='<b>%{x}</b><br>新聞數量: %{y}<extra></extra>'\n",
        "                )\n",
        "            ])\n",
        "\n",
        "            fig.update_layout(\n",
        "                title='<b>新聞來源分布</b>',\n",
        "                xaxis_title='新聞來源',\n",
        "                yaxis_title='新聞數量',\n",
        "                height=400,\n",
        "                template='plotly_white',\n",
        "                showlegend=False\n",
        "            )\n",
        "\n",
        "            print(f\"✅ 時間軸生成完成\")\n",
        "            return fig\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ 時間軸生成失敗: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def create_comprehensive_dashboard(self, analysis_results, news_data):\n",
        "        \"\"\"建立綜合儀表板\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"🎨 開始生成視覺化儀表板\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        dashboard = {}\n",
        "\n",
        "        # 1. 文字雲（傳統方式）\n",
        "        if 'sklearn_tfidf' in analysis_results and analysis_results['sklearn_tfidf']:\n",
        "            dashboard['wordcloud'] = self.create_wordcloud(\n",
        "                analysis_results['sklearn_tfidf'],\n",
        "                title=\"前50熱門關鍵字文字雲\",\n",
        "                max_words=50\n",
        "            )\n",
        "\n",
        "        # 2. Plotly 文字雲（備用方案）\n",
        "        if 'sklearn_tfidf' in analysis_results and analysis_results['sklearn_tfidf']:\n",
        "            dashboard['wordcloud_plotly'] = self.create_wordcloud_plotly(\n",
        "                analysis_results['sklearn_tfidf'],\n",
        "                title=\"關鍵字視覺化（Plotly版）\",\n",
        "                max_words=50\n",
        "            )\n",
        "\n",
        "        # 3. 關鍵字橫條圖\n",
        "        if 'sklearn_tfidf' in analysis_results and analysis_results['sklearn_tfidf']:\n",
        "            dashboard['bar_chart'] = self.plot_top_keywords_bar(\n",
        "                analysis_results['sklearn_tfidf'],\n",
        "                title=\"前20關鍵字排行榜\",\n",
        "                top_n=20\n",
        "            )\n",
        "\n",
        "        # 4. 圓餅圖\n",
        "        if 'sklearn_tfidf' in analysis_results and analysis_results['sklearn_tfidf']:\n",
        "            dashboard['pie_chart'] = self.plot_keyword_distribution_pie(\n",
        "                analysis_results['sklearn_tfidf'],\n",
        "                title=\"前15關鍵字分數分布\",\n",
        "                top_n=15\n",
        "            )\n",
        "\n",
        "        # 5. 方法比較圖\n",
        "        dashboard['comparison'] = self.plot_methods_comparison(analysis_results, top_n=10)\n",
        "\n",
        "        # 6. 熱力圖\n",
        "        dashboard['heatmap'] = self.plot_all_methods_heatmap(analysis_results, top_n=15)\n",
        "\n",
        "        # 7. 網絡圖\n",
        "        if 'sklearn_tfidf' in analysis_results and analysis_results['sklearn_tfidf']:\n",
        "            dashboard['network'] = self.plot_keyword_network(\n",
        "                analysis_results['sklearn_tfidf'],\n",
        "                top_n=20\n",
        "            )\n",
        "\n",
        "        # 8. 新聞時間軸\n",
        "        if news_data:\n",
        "            dashboard['timeline'] = self.plot_news_timeline(news_data)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"✅ 視覺化儀表板生成完成\")\n",
        "        print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "        return dashboard"
      ],
      "metadata": {
        "id": "gQUuba49lu1y"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "VkExqssCMwU5"
      },
      "outputs": [],
      "source": [
        "# ==================== Google Sheets 操作 ====================\n",
        "class SheetManager:\n",
        "    \"\"\"Google Sheets 管理\"\"\"\n",
        "\n",
        "    def __init__(self, gc, sheet_url):\n",
        "        self.gc = gc\n",
        "        self.spreadsheet = gc.open_by_url(sheet_url)\n",
        "\n",
        "    def write_news_data(self, news_data):\n",
        "        \"\"\"寫入新聞資料到 Sheet\"\"\"\n",
        "        try:\n",
        "            # 取得或建立工作表\n",
        "            try:\n",
        "                sheet = self.spreadsheet.worksheet('新聞資料')\n",
        "            except:\n",
        "                sheet = self.spreadsheet.add_worksheet(title='新聞資料', rows=1000, cols=10)\n",
        "\n",
        "            # 清空並寫入標題\n",
        "            sheet.clear()\n",
        "            headers = ['標題', '摘要', '連結', '發布時間', '爬取時間', '來源']\n",
        "            sheet.append_row(headers)\n",
        "\n",
        "            # 寫入資料\n",
        "            for news in news_data:\n",
        "                row = [\n",
        "                    news.get('標題', ''),\n",
        "                    news.get('摘要', ''),\n",
        "                    news.get('連結', ''),\n",
        "                    news.get('發布時間', ''),\n",
        "                    news.get('爬取時間', ''),\n",
        "                    news.get('來源', '')\n",
        "                ]\n",
        "                sheet.append_row(row)\n",
        "\n",
        "            return f\"✅ 成功寫入 {len(news_data)} 則新聞到 Google Sheet\"\n",
        "        except Exception as e:\n",
        "            return f\"❌ 寫入失敗: {str(e)}\"\n",
        "\n",
        "    def read_news_data(self):\n",
        "        \"\"\"從 Sheet 讀取新聞資料\"\"\"\n",
        "        try:\n",
        "            sheet = self.spreadsheet.worksheet('新聞資料')\n",
        "            data = sheet.get_all_records()\n",
        "            return data\n",
        "        except Exception as e:\n",
        "            print(f\"❌ 讀取失敗: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def write_keywords(self, keywords):\n",
        "        \"\"\"寫入關鍵字統計到 Sheet\"\"\"\n",
        "        try:\n",
        "            # 取得或建立工作表\n",
        "            try:\n",
        "                sheet = self.spreadsheet.worksheet('關鍵字統計')\n",
        "            except:\n",
        "                sheet = self.spreadsheet.add_worksheet(title='關鍵字統計', rows=100, cols=5)\n",
        "\n",
        "            # 清空並寫入標題\n",
        "            sheet.clear()\n",
        "            headers = ['排名', '關鍵字', 'TF-IDF分數', '更新時間']\n",
        "            sheet.append_row(headers)\n",
        "\n",
        "            # 寫入資料\n",
        "            for idx, (keyword, score) in enumerate(keywords, 1):\n",
        "                row = [\n",
        "                    idx,\n",
        "                    keyword,\n",
        "                    round(score, 4),\n",
        "                    datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                ]\n",
        "                sheet.append_row(row)\n",
        "\n",
        "            return f\"✅ 成功寫入 {len(keywords)} 個關鍵字到統計表\"\n",
        "        except Exception as e:\n",
        "            return f\"❌ 寫入關鍵字失敗: {str(e)}\"\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== Gemini AI 分析 ====================\n",
        "def generate_insights_with_gemini(model, news_data, keywords):\n",
        "    \"\"\"使用 Gemini 生成洞察摘要\"\"\"\n",
        "    try:\n",
        "        # 準備提示詞\n",
        "        keywords_text = ', '.join([kw for kw, _ in keywords[:10]])\n",
        "        news_titles = '\\n'.join([f\"- {news['標題']}\" for news in news_data[:20]])\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "你是專業的金融分析師。根據以下台股新聞資料，請提供分析：\n",
        "\n",
        "【新聞標題】（共 {len(news_data)} 則）\n",
        "{news_titles}\n",
        "\n",
        "【關鍵字】\n",
        "{keywords_text}\n",
        "\n",
        "請提供：\n",
        "1. **5 句關鍵洞察**（每句 30 字內，分點條列）\n",
        "2. **總結論**（120 字，深入分析市場趨勢與投資建議）\n",
        "\n",
        "格式如下：\n",
        "## 關鍵洞察\n",
        "1. [洞察1]\n",
        "2. [洞察2]\n",
        "3. [洞察3]\n",
        "4. [洞察4]\n",
        "5. [洞察5]\n",
        "\n",
        "## 總結\n",
        "[120字總結]\n",
        "\"\"\"\n",
        "\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"❌ Gemini 分析失敗: {str(e)}\\n\\n請檢查 API Key 是否正確\""
      ],
      "metadata": {
        "id": "Ujr5jnUgNFac"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 主流程整合（加入視覺化）====================\n",
        "def automated_pipeline(sheet_url, gemini_api_key, crawl_pages=3, analysis_method='comprehensive'):\n",
        "    \"\"\"完整自動化流程 - 加入視覺化\"\"\"\n",
        "\n",
        "    results = {\n",
        "        'status': [],\n",
        "        'news_data': None,\n",
        "        'keywords': None,\n",
        "        'analysis_details': None,\n",
        "        'insights': None,\n",
        "        'visualizations': None\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Step 1: 爬取新聞\n",
        "        results['status'].append(\"🔍 步驟1: 開始爬取財經新聞...\")\n",
        "        crawler = FinanceNewsCrawler()\n",
        "        news_data = crawler.auto_crawl(max_news=min(30, crawl_pages * 10))\n",
        "\n",
        "        if not news_data:\n",
        "            results['status'].append(\"❌ 爬取失敗，請檢查網路連線\")\n",
        "            return results\n",
        "\n",
        "        results['news_data'] = news_data\n",
        "        results['status'].append(f\"✅ 成功爬取 {len(news_data)} 則新聞\")\n",
        "\n",
        "        # Step 2: 寫入 Google Sheet\n",
        "        results['status'].append(\"📝 步驟2: 寫入 Google Sheet...\")\n",
        "        gc = setup_google_sheets()\n",
        "        sheet_manager = SheetManager(gc, sheet_url)\n",
        "        write_result = sheet_manager.write_news_data(news_data)\n",
        "        results['status'].append(write_result)\n",
        "\n",
        "        # Step 3: 使用結巴分詞進行文字分析\n",
        "        results['status'].append(\"🔬 步驟3: 使用結巴分詞進行文字分析...\")\n",
        "        analyzer = TextAnalyzer()\n",
        "\n",
        "        # 合併標題和摘要\n",
        "        texts = [f\"{n['標題']} {n.get('摘要', '')}\" for n in news_data]\n",
        "\n",
        "        # 執行綜合分析\n",
        "        if analysis_method == 'comprehensive':\n",
        "            results['status'].append(\"📊 執行綜合分析（詞頻 + TF-IDF + TextRank）...\")\n",
        "            analysis_results = analyzer.comprehensive_analysis(texts, top_n=50)  # 提取前50個\n",
        "            results['analysis_details'] = analysis_results\n",
        "\n",
        "            # 使用 TF-IDF 作為主要關鍵字\n",
        "            keywords = analysis_results.get('sklearn_tfidf', [])\n",
        "            if not keywords:\n",
        "                keywords = analysis_results.get('jieba_tfidf', [])\n",
        "        else:\n",
        "            keywords = analyzer.get_best_keywords(texts, top_n=50, method=analysis_method)\n",
        "\n",
        "        results['keywords'] = keywords\n",
        "        results['status'].append(f\"✅ 提取前 50 個關鍵字\")\n",
        "\n",
        "        # Step 4: 回寫關鍵字到統計表\n",
        "        results['status'].append(\"📊 步驟4: 回寫關鍵字統計...\")\n",
        "        keyword_result = sheet_manager.write_keywords(keywords[:10])  # 只寫入前10個\n",
        "        results['status'].append(keyword_result)\n",
        "\n",
        "        # Step 4.5: 回寫詳細分析結果\n",
        "        if analysis_method == 'comprehensive' and results['analysis_details']:\n",
        "            results['status'].append(\"📊 步驟4.5: 回寫詳細分析結果...\")\n",
        "            try:\n",
        "                sheet_manager.write_comprehensive_analysis(results['analysis_details'])\n",
        "                results['status'].append(\"✅ 詳細分析結果已寫入\")\n",
        "            except Exception as e:\n",
        "                results['status'].append(f\"⚠️ 詳細分析寫入失敗: {str(e)}\")\n",
        "\n",
        "        # Step 5: 生成視覺化圖表\n",
        "        results['status'].append(\"🎨 步驟5: 生成視覺化圖表...\")\n",
        "        visualizer = DataVisualizer()\n",
        "\n",
        "        if analysis_method == 'comprehensive' and results['analysis_details']:\n",
        "            visualizations = visualizer.create_comprehensive_dashboard(\n",
        "                results['analysis_details'],\n",
        "                news_data\n",
        "            )\n",
        "        else:\n",
        "            # 簡化版視覺化\n",
        "            visualizations = {\n",
        "                'wordcloud': visualizer.create_wordcloud(keywords, max_words=50),\n",
        "                'bar_chart': visualizer.plot_top_keywords_bar(keywords, top_n=20),\n",
        "                'pie_chart': visualizer.plot_keyword_distribution_pie(keywords, top_n=15),\n",
        "                'network': visualizer.plot_keyword_network(keywords, top_n=20),\n",
        "                'timeline': visualizer.plot_news_timeline(news_data)\n",
        "            }\n",
        "\n",
        "        results['visualizations'] = visualizations\n",
        "        results['status'].append(\"✅ 視覺化圖表生成完成\")\n",
        "\n",
        "        # Step 6: 使用 Gemini 生成洞察\n",
        "        if gemini_api_key and gemini_api_key.strip():\n",
        "            results['status'].append(\"🤖 步驟6: 使用 Gemini AI 生成洞察...\")\n",
        "            try:\n",
        "                model = setup_gemini(gemini_api_key)\n",
        "                insights = generate_insights_with_gemini(model, news_data, keywords[:10], results.get('analysis_details'))\n",
        "                results['insights'] = insights\n",
        "                results['status'].append(\"✅ Gemini 分析完成\")\n",
        "            except Exception as e:\n",
        "                results['insights'] = f\"❌ Gemini 分析失敗: {str(e)}\\n請檢查 API Key 是否正確\"\n",
        "                results['status'].append(\"⚠️ Gemini 分析失敗（可能是 API Key 錯誤）\")\n",
        "        else:\n",
        "            results['insights'] = \"⚠️ 未提供 Gemini API Key，跳過 AI 分析\"\n",
        "            results['status'].append(\"⚠️ 跳過 Gemini 分析（未提供 API Key）\")\n",
        "\n",
        "        results['status'].append(\"\\n🎉 所有步驟完成！\")\n",
        "\n",
        "    except Exception as e:\n",
        "        results['status'].append(f\"❌ 流程錯誤: {str(e)}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "kiCdFg4cNKpn"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== Gradio 介面（文字雲修正版）====================\n",
        "def create_gradio_interface():\n",
        "    \"\"\"建立 Gradio 介面 - 文字雲修正版\"\"\"\n",
        "\n",
        "    def run_pipeline(sheet_url, gemini_key, pages):\n",
        "        \"\"\"執行完整流程\"\"\"\n",
        "        results = automated_pipeline(sheet_url, gemini_key, int(pages), 'comprehensive')\n",
        "\n",
        "        # 狀態訊息\n",
        "        status_text = '\\n'.join(results['status'])\n",
        "\n",
        "        # 關鍵字表格\n",
        "        if results['keywords']:\n",
        "            keywords_df = pd.DataFrame(\n",
        "                results['keywords'][:20],\n",
        "                columns=['關鍵字', 'TF-IDF分數']\n",
        "            )\n",
        "            keywords_df.index = range(1, len(keywords_df) + 1)\n",
        "            keywords_df['TF-IDF分數'] = keywords_df['TF-IDF分數'].round(4)\n",
        "        else:\n",
        "            keywords_df = pd.DataFrame()\n",
        "\n",
        "        # 新聞表格\n",
        "        if results['news_data']:\n",
        "            news_df = pd.DataFrame(results['news_data'])\n",
        "            news_df = news_df[['標題', '發布時間', '來源', '連結']]\n",
        "        else:\n",
        "            news_df = pd.DataFrame()\n",
        "\n",
        "        # Gemini 洞察\n",
        "        insights_text = results['insights'] if results['insights'] else \"尚未生成分析\"\n",
        "\n",
        "        # 視覺化圖表\n",
        "        viz = results.get('visualizations', {})\n",
        "\n",
        "        wordcloud_img = viz.get('wordcloud')  # 傳統文字雲（圖片）\n",
        "        wordcloud_plotly = viz.get('wordcloud_plotly')  # Plotly文字雲（互動圖表）\n",
        "        bar_chart = viz.get('bar_chart')\n",
        "        pie_chart = viz.get('pie_chart')\n",
        "        comparison_chart = viz.get('comparison')\n",
        "        heatmap_chart = viz.get('heatmap')\n",
        "        network_chart = viz.get('network')\n",
        "        timeline_chart = viz.get('timeline')\n",
        "\n",
        "        return (\n",
        "            status_text,\n",
        "            keywords_df,\n",
        "            news_df,\n",
        "            insights_text,\n",
        "            wordcloud_img,  # 傳統文字雲\n",
        "            wordcloud_plotly,  # Plotly文字雲\n",
        "            bar_chart,\n",
        "            pie_chart,\n",
        "            comparison_chart,\n",
        "            heatmap_chart,\n",
        "            network_chart,\n",
        "            timeline_chart\n",
        "        )\n",
        "\n",
        "    # 建立介面\n",
        "    with gr.Blocks(title=\"🚀 台股財經新聞分析系統\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"\"\"\n",
        "        # 🚀 台股財經新聞自動化分析系統\n",
        "        ### 📊 爬蟲 → Google Sheet → 結巴分詞 → TF-IDF → Gemini AI → 視覺化儀表板\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                sheet_url_input = gr.Textbox(\n",
        "                    label=\"📄 Google Sheet 網址\",\n",
        "                    placeholder=\"https://docs.google.com/spreadsheets/d/YOUR_SHEET_ID/edit\",\n",
        "                    value=\"https://docs.google.com/spreadsheets/d/1UzNgDMKD_WH3uhfQXdrBPM_z8oFHLh77yahMbu5KlHo/edit?usp=sharing\"\n",
        "                )\n",
        "            with gr.Column(scale=2):\n",
        "                gemini_key_input = gr.Textbox(\n",
        "                    label=\"🔑 Gemini API Key (選填)\",\n",
        "                    placeholder=\"輸入你的 Gemini API Key 或留空跳過 AI 分析\",\n",
        "                    type=\"password\"\n",
        "                )\n",
        "            with gr.Column(scale=1):\n",
        "                pages_input = gr.Number(\n",
        "                    label=\"📄 爬取頁數\",\n",
        "                    value=3,\n",
        "                    minimum=1,\n",
        "                    maximum=5\n",
        "                )\n",
        "\n",
        "        run_button = gr.Button(\"🚀 開始執行完整流程\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "\n",
        "        with gr.Tabs():\n",
        "            with gr.Tab(\"📋 執行狀態\"):\n",
        "                status_output = gr.Textbox(\n",
        "                    label=\"流程狀態\",\n",
        "                    lines=20,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "            with gr.Tab(\"🔑 關鍵字分析\"):\n",
        "                gr.Markdown(\"### 📊 前 20 熱門關鍵字 (TF-IDF 分析)\")\n",
        "                keywords_output = gr.Dataframe(\n",
        "                    label=\"關鍵字排行\",\n",
        "                    headers=['關鍵字', 'TF-IDF分數'],\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "            with gr.Tab(\"📰 新聞列表\"):\n",
        "                gr.Markdown(\"### 📰 爬取的新聞資料\")\n",
        "                news_output = gr.Dataframe(\n",
        "                    label=\"新聞列表\",\n",
        "                    headers=['標題', '發布時間', '來源', '連結'],\n",
        "                    interactive=False,\n",
        "                    wrap=True\n",
        "                )\n",
        "\n",
        "            with gr.Tab(\"🤖 AI 洞察\"):\n",
        "                gr.Markdown(\"### 🤖 Gemini AI 深度分析\")\n",
        "                insights_output = gr.Markdown(value=\"等待執行...\")\n",
        "\n",
        "            with gr.Tab(\"☁️ 文字雲（傳統）\"):\n",
        "                gr.Markdown(\"### ☁️ 前50熱門關鍵字文字雲（WordCloud）\")\n",
        "                gr.Markdown(\"使用 WordCloud 套件生成，字體大小代表重要性\")\n",
        "                wordcloud_output = gr.Image(label=\"關鍵字文字雲\", type=\"filepath\")\n",
        "\n",
        "            with gr.Tab(\"☁️ 文字雲（互動）\"):\n",
        "                gr.Markdown(\"### ☁️ 關鍵字視覺化（Plotly 互動版）\")\n",
        "                gr.Markdown(\"可縮放、拖曳、懸停查看分數\")\n",
        "                wordcloud_plotly_output = gr.Plot(label=\"Plotly 文字雲\")\n",
        "\n",
        "            with gr.Tab(\"📊 橫條圖\"):\n",
        "                gr.Markdown(\"### 📊 前 20 關鍵字排行榜\")\n",
        "                bar_output = gr.Plot(label=\"關鍵字橫條圖\")\n",
        "\n",
        "            with gr.Tab(\"🥧 圓餅圖\"):\n",
        "                gr.Markdown(\"### 🥧 關鍵字分數分布\")\n",
        "                pie_output = gr.Plot(label=\"關鍵字圓餅圖\")\n",
        "\n",
        "            with gr.Tab(\"🔀 方法比較\"):\n",
        "                gr.Markdown(\"### 🔀 不同分析方法的關鍵字比較\")\n",
        "                comparison_output = gr.Plot(label=\"分析方法比較\")\n",
        "\n",
        "            with gr.Tab(\"🔥 熱力圖\"):\n",
        "                gr.Markdown(\"### 🔥 關鍵字分析熱力圖\")\n",
        "                heatmap_output = gr.Plot(label=\"熱力圖\")\n",
        "\n",
        "            with gr.Tab(\"🕸️ 網絡圖\"):\n",
        "                gr.Markdown(\"### 🕸️ 關鍵字網絡圖（泡泡圖）\")\n",
        "                network_output = gr.Plot(label=\"關鍵字網絡圖\")\n",
        "\n",
        "            with gr.Tab(\"📈 新聞分布\"):\n",
        "                gr.Markdown(\"### 📈 新聞來源分布統計\")\n",
        "                timeline_output = gr.Plot(label=\"新聞來源統計\")\n",
        "\n",
        "        # 綁定按鈕\n",
        "        run_button.click(\n",
        "            run_pipeline,\n",
        "            inputs=[sheet_url_input, gemini_key_input, pages_input],\n",
        "            outputs=[\n",
        "                status_output,\n",
        "                keywords_output,\n",
        "                news_output,\n",
        "                insights_output,\n",
        "                wordcloud_output,  # 傳統文字雲\n",
        "                wordcloud_plotly_output,  # Plotly文字雲\n",
        "                bar_output,\n",
        "                pie_output,\n",
        "                comparison_output,\n",
        "                heatmap_output,\n",
        "                network_output,\n",
        "                timeline_output\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ---\n",
        "        ### 💡 使用說明\n",
        "\n",
        "        現在提供**兩種文字雲**：\n",
        "        1. **☁️ 傳統文字雲**: 使用 WordCloud 套件，視覺效果最佳\n",
        "        2. ☁️ 互動文字雲: 使用 Plotly，可縮放、拖曳、懸停\n",
        "\n",
        "        #### 🎨 視覺化圖表\n",
        "        - ☁️ **文字雲（兩種）**: 前50個關鍵字視覺化\n",
        "        - 📊 **橫條圖**: 前20個關鍵字排行\n",
        "        - 🥧 **圓餅圖**: 前15個關鍵字佔比\n",
        "        - 🔀 **方法比較**: 4種分析方法對比\n",
        "        - 🔥 **熱力圖**: 關鍵字×方法矩陣\n",
        "        - 🕸️ **網絡圖**: 泡泡大小=重要性\n",
        "        - 📈 **新聞分布**: 來源統計\n",
        "        \"\"\")\n",
        "\n",
        "        return demo"
      ],
      "metadata": {
        "id": "J91VFUZfNPd7"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 啟動應用 ====================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🚀 正在啟動台股財經新聞分析系統...\")\n",
        "    print(\"📊 包含完整視覺化儀表板\")\n",
        "    print(\"=\" * 60)\n",
        "    app = create_gradio_interface()\n",
        "    app.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "goGwth11NSxq",
        "outputId": "14eec79b-9d90-4152-ce2e-07f5e2badbe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 正在啟動台股財經新聞分析系統...\n",
            "📊 包含完整視覺化儀表板\n",
            "============================================================\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://4ac96dcf9f5151531a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4ac96dcf9f5151531a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 開始智慧爬蟲...\n",
            "📡 嘗試使用: RSS Feed\n",
            "🔍 正在爬取聯合新聞網 RSS...\n",
            "✅ 成功爬取 30 則新聞（聯合新聞網）\n",
            "✅ 成功！使用 RSS Feed 取得 30 則新聞\n",
            "🔧 初始化結巴分詞...\n",
            "✅ 結巴分詞初始化完成\n",
            "📚 停用詞數量: 136 個\n",
            "📖 自訂詞彙數量: 98 個\n",
            "\n",
            "============================================================\n",
            "🚀 開始綜合文字分析（已過濾網址和雜訊）\n",
            "============================================================\n",
            "\n",
            "📊 方法1: 詞頻統計\n",
            "🔍 進行詞頻統計...\n",
            "✅ 統計了 137 個不同的詞，返回前 50 個\n",
            "\n",
            "【詞頻統計】前 10 個關鍵字:\n",
            "  1. img (23.0000)\n",
            "  2. src (23.0000)\n",
            "  3. 台積電 (10.0000)\n",
            "  4. 聯準會 (3.0000)\n",
            "  5. Fed (3.0000)\n",
            "  6. 半導體 (3.0000)\n",
            "  7. 人壽成 (2.0000)\n",
            "  8. 贊助夥伴 (2.0000)\n",
            "  9. TPVL (2.0000)\n",
            "  10. 徵台灣 (2.0000)\n",
            "\n",
            "📊 方法2: jieba TF-IDF\n",
            "🔍 使用 jieba.analyse 提取關鍵字...\n",
            "✅ 提取了 9 個有效關鍵字\n",
            "\n",
            "【jieba TF-IDF】前 10 個關鍵字:\n",
            "  1. 聯邦銀行 (0.0373)\n",
            "  2. 記憶體 (0.0373)\n",
            "  3. 杜金龍 (0.0373)\n",
            "  4. 陶朱隱 (0.0373)\n",
            "  5. 黃立成 (0.0373)\n",
            "  6. 伺服器 (0.0189)\n",
            "  7. 內政部 (0.0187)\n",
            "  8. 停電降 (0.0187)\n",
            "  9. 指數續 (0.0187)\n",
            "\n",
            "📊 方法3: TextRank 演算法\n",
            "🔍 使用 TextRank 演算法提取關鍵字...\n",
            "✅ 提取了 7 個有效關鍵字\n",
            "\n",
            "【TextRank】前 10 個關鍵字:\n",
            "  1. 陶朱隱 (0.4135)\n",
            "  2. 黃立成 (0.3885)\n",
            "  3. 記憶體 (0.3666)\n",
            "  4. 概念股 (0.2595)\n",
            "  5. 內政部 (0.2541)\n",
            "  6. 信義區 (0.2336)\n",
            "  7. 杜金龍 (0.2318)\n",
            "\n",
            "📊 方法4: sklearn TF-IDF\n",
            "🔍 使用 TF-IDF 分析關鍵字...\n",
            "✅ TF-IDF 分析完成，提取 50 個關鍵字\n",
            "\n",
            "【sklearn TF-IDF】前 10 個關鍵字:\n",
            "  1. img (3.6144)\n",
            "  2. src (3.6144)\n",
            "  3. 台積電 (2.4812)\n",
            "  4. fed (1.0356)\n",
            "  5. 聯準會 (1.0356)\n",
            "  6. 覆蓋率 (1.0000)\n",
            "  7. 半導體 (0.9688)\n",
            "  8. 籲積極 (0.9033)\n",
            "  9. 加權指數 (0.8218)\n",
            "  10. 三大法人 (0.7834)\n",
            "\n",
            "============================================================\n",
            "✅ 綜合分析完成\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "🎨 開始生成視覺化儀表板\n",
            "============================================================\n",
            "🎨 生成文字雲: 前50熱門關鍵字文字雲\n",
            "❌ 文字雲生成失敗: cannot open resource\n",
            "🎨 生成 Plotly 文字雲: 關鍵字視覺化（Plotly版）\n",
            "✅ Plotly 文字雲生成完成\n",
            "📊 生成關鍵字橫條圖: 前20關鍵字排行榜\n",
            "✅ 橫條圖生成完成 (20 個關鍵字)\n",
            "📊 生成關鍵字圓餅圖: 前15關鍵字分數分布\n",
            "✅ 圓餅圖生成完成\n",
            "📊 生成分析方法比較圖\n",
            "✅ 方法比較圖生成完成\n",
            "📊 生成分析方法熱力圖\n",
            "✅ 熱力圖生成完成\n",
            "📊 生成關鍵字網絡圖\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3580048705.py\", line 45, in create_wordcloud\n",
            "    ).generate_from_frequencies(word_freq)\n",
            "      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/wordcloud/wordcloud.py\", line 453, in generate_from_frequencies\n",
            "    self.generate_from_frequencies(dict(frequencies[:2]),\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/wordcloud/wordcloud.py\", line 506, in generate_from_frequencies\n",
            "    font = ImageFont.truetype(self.font_path, font_size)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/PIL/ImageFont.py\", line 880, in truetype\n",
            "    return freetype(font)\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/PIL/ImageFont.py\", line 877, in freetype\n",
            "    return FreeTypeFont(font, size, index, encoding, layout_engine)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/PIL/ImageFont.py\", line 285, in __init__\n",
            "    self.font = core.getfont(\n",
            "                ^^^^^^^^^^^^^\n",
            "OSError: cannot open resource\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 網絡圖生成完成\n",
            "📊 生成新聞時間軸\n",
            "✅ 時間軸生成完成\n",
            "\n",
            "============================================================\n",
            "✅ 視覺化儀表板生成完成\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}