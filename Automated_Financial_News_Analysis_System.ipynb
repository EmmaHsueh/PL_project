{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoUuqjzL4PjYHP+dbW5/tS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmmaHsueh/PL_project/blob/main/Automated_Financial_News_Analysis_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q requests beautifulsoup4 pandas gradio google-generativeai gspread oauth2client scikit-learn jieba feedparser wordcloud matplotlib plotly kaleido"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei5PbjIsM0Iv",
        "outputId": "61875a3d-d44b-4da9-b6ab-bcfa3c1ba069"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/66.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/56.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==================== åŒ¯å…¥å¥—ä»¶ ====================\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import gradio as gr\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from google.auth import default\n",
        "import google.generativeai as genai\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import jieba\n",
        "import jieba.analyse\n",
        "from collections import Counter\n",
        "import re\n",
        "import feedparser\n"
      ],
      "metadata": {
        "id": "lfqywNcCM1q7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==================== Google Sheets è¨­å®š ====================\n",
        "def setup_google_sheets():\n",
        "    \"\"\"è¨­å®š Google Sheets é€£ç·š\"\"\"\n",
        "    auth.authenticate_user()\n",
        "    creds, _ = default()\n",
        "    gc = gspread.authorize(creds)\n",
        "    return gc\n"
      ],
      "metadata": {
        "id": "QU33teX7M23c"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==================== Gemini API è¨­å®š ====================\n",
        "def setup_gemini(api_key):\n",
        "    \"\"\"è¨­å®š Gemini API\"\"\"\n",
        "    genai.configure(api_key=api_key)\n",
        "    model = genai.GenerativeModel('models/gemini-2.5-pro')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "up8sO6j6M4lG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==================== è²¡ç¶“æ–°èçˆ¬èŸ²ï¼ˆæ”¹è‰¯ç‰ˆï¼‰====================\n",
        "class FinanceNewsCrawler:\n",
        "    \"\"\"çˆ¬å–å°è‚¡è²¡ç¶“æ–°è - ä½¿ç”¨å¤šç¨®ç©©å®šä¾†æº\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',\n",
        "        }\n",
        "        self.news_data = []\n",
        "\n",
        "    def crawl_udn_rss(self, max_news=30):\n",
        "        \"\"\"æ–¹æ³•1: çˆ¬å–è¯åˆæ–°èç¶² RSSï¼ˆæœ€ç©©å®šï¼‰\"\"\"\n",
        "        self.news_data = []\n",
        "\n",
        "        try:\n",
        "            print(\"ğŸ” æ­£åœ¨çˆ¬å–è¯åˆæ–°èç¶² RSS...\")\n",
        "            rss_urls = [\n",
        "                'https://udn.com/rssfeed/news/2/6644?ch=news',  # è‚¡å¸‚è¦è\n",
        "                'https://udn.com/rssfeed/news/2/6645?ch=news',  # ä¸Šå¸‚é›»å­\n",
        "            ]\n",
        "\n",
        "            for rss_url in rss_urls:\n",
        "                try:\n",
        "                    feed = feedparser.parse(rss_url)\n",
        "\n",
        "                    for entry in feed.entries[:15]:\n",
        "                        self.news_data.append({\n",
        "                            'æ¨™é¡Œ': entry.title,\n",
        "                            'æ‘˜è¦': entry.summary if hasattr(entry, 'summary') else '',\n",
        "                            'é€£çµ': entry.link,\n",
        "                            'ç™¼å¸ƒæ™‚é–“': entry.published if hasattr(entry, 'published') else 'æœªçŸ¥æ™‚é–“',\n",
        "                            'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                            'ä¾†æº': 'è¯åˆæ–°èç¶²'\n",
        "                        })\n",
        "\n",
        "                        if len(self.news_data) >= max_news:\n",
        "                            break\n",
        "\n",
        "                    time.sleep(1)\n",
        "                except Exception as e:\n",
        "                    print(f\"âš ï¸ RSS çˆ¬å–éƒ¨åˆ†å¤±æ•—: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            print(f\"âœ… æˆåŠŸçˆ¬å– {len(self.news_data)} å‰‡æ–°èï¼ˆè¯åˆæ–°èç¶²ï¼‰\")\n",
        "            return self.news_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ RSS çˆ¬å–å¤±æ•—: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def crawl_ctee_news(self, max_news=30):\n",
        "        \"\"\"æ–¹æ³•2: çˆ¬å–å·¥å•†æ™‚å ±ï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰\"\"\"\n",
        "        self.news_data = []\n",
        "\n",
        "        try:\n",
        "            print(\"ğŸ” æ­£åœ¨çˆ¬å–å·¥å•†æ™‚å ±...\")\n",
        "            url = \"https://ctee.com.tw/news/stock\"\n",
        "\n",
        "            response = requests.get(url, headers=self.headers, timeout=15)\n",
        "            response.encoding = 'utf-8'\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # æ‰¾åˆ°æ–°èåˆ—è¡¨\n",
        "            news_items = soup.find_all('div', class_='item')\n",
        "\n",
        "            for item in news_items[:max_news]:\n",
        "                try:\n",
        "                    title_tag = item.find('h3')\n",
        "                    link_tag = item.find('a')\n",
        "                    time_tag = item.find('time')\n",
        "\n",
        "                    if title_tag and link_tag:\n",
        "                        title = title_tag.get_text(strip=True)\n",
        "                        link = link_tag['href'] if link_tag['href'].startswith('http') else 'https://ctee.com.tw' + link_tag['href']\n",
        "                        publish_time = time_tag.get_text(strip=True) if time_tag else 'æœªçŸ¥æ™‚é–“'\n",
        "\n",
        "                        self.news_data.append({\n",
        "                            'æ¨™é¡Œ': title,\n",
        "                            'æ‘˜è¦': '',\n",
        "                            'é€£çµ': link,\n",
        "                            'ç™¼å¸ƒæ™‚é–“': publish_time,\n",
        "                            'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                            'ä¾†æº': 'å·¥å•†æ™‚å ±'\n",
        "                        })\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "\n",
        "            print(f\"âœ… æˆåŠŸçˆ¬å– {len(self.news_data)} å‰‡æ–°èï¼ˆå·¥å•†æ™‚å ±ï¼‰\")\n",
        "            return self.news_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ å·¥å•†æ™‚å ±çˆ¬å–å¤±æ•—: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def crawl_demo_data(self):\n",
        "        \"\"\"æ–¹æ³•3: ä½¿ç”¨ç¤ºç¯„è³‡æ–™ï¼ˆä¿è­‰å¯åŸ·è¡Œï¼‰\"\"\"\n",
        "        print(\"ğŸ” ä½¿ç”¨ç¤ºç¯„æ–°èè³‡æ–™...\")\n",
        "\n",
        "        self.news_data = [\n",
        "            {'æ¨™é¡Œ': 'å°ç©é›»æ³•èªªæœƒé‡‹åˆ©å¤š å¤–è³‡ç›®æ¨™åƒ¹ä¸Šçœ‹1200å…ƒ', 'æ‘˜è¦': 'å°ç©é›»èˆ‰è¡Œæ³•èªªæœƒï¼Œé‡‹å‡ºå¤šé …åˆ©å¤šæ¶ˆæ¯ï¼Œå¤–è³‡çœ‹å¥½å¾Œå¸‚', 'é€£çµ': 'https://example.com/1', 'ç™¼å¸ƒæ™‚é–“': '2025-10-26 10:30', 'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'ä¾†æº': 'ç¤ºç¯„è³‡æ–™'},\n",
        "            {'æ¨™é¡Œ': 'ç¾è‚¡ç§‘æŠ€è‚¡å¤§æ¼² å°è‚¡é›»å­è‚¡å¯æœ›å—æƒ ', 'æ‘˜è¦': 'ç¾åœ‹ç§‘æŠ€è‚¡å¼·å‹ä¸Šæ¼²ï¼Œå¸¶å‹•å°ç£é›»å­è‚¡è²·æ°£', 'é€£çµ': 'https://example.com/2', 'ç™¼å¸ƒæ™‚é–“': '2025-10-26 09:15', 'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'ä¾†æº': 'ç¤ºç¯„è³‡æ–™'},\n",
        "            {'æ¨™é¡Œ': 'AIéœ€æ±‚å¼·å‹ åŠå°é«”ç”¢æ¥­éˆç‡Ÿæ”¶å‰µæ–°é«˜', 'æ‘˜è¦': 'äººå·¥æ™ºæ…§æ‡‰ç”¨å¸¶å‹•åŠå°é«”éœ€æ±‚å¤§å¢', 'é€£çµ': 'https://example.com/3', 'ç™¼å¸ƒæ™‚é–“': '2025-10-26 08:45', 'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'ä¾†æº': 'ç¤ºç¯„è³‡æ–™'},\n",
        "            {'æ¨™é¡Œ': 'å°è‚¡ç«™ç©©è¬å…« å¤–è³‡é€£çºŒè²·è¶…', 'æ‘˜è¦': 'å°ç£è‚¡å¸‚è¡¨ç¾å¼·å‹ï¼Œå¤–è³‡æŒçºŒè²·å…¥', 'é€£çµ': 'https://example.com/4', 'ç™¼å¸ƒæ™‚é–“': '2025-10-25 16:20', 'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'ä¾†æº': 'ç¤ºç¯„è³‡æ–™'},\n",
        "            {'æ¨™é¡Œ': 'è¯ç™¼ç§‘æ¨å‡ºæ–°æ¬¾5Gæ™¶ç‰‡ ç²å¸‚å ´å¥½è©•', 'æ‘˜è¦': 'è¯ç™¼ç§‘æœ€æ–°æ™¶ç‰‡æ•ˆèƒ½æå‡ï¼Œé æœŸç‡Ÿæ”¶æˆé•·', 'é€£çµ': 'https://example.com/5', 'ç™¼å¸ƒæ™‚é–“': '2025-10-25 14:30', 'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'ä¾†æº': 'ç¤ºç¯„è³‡æ–™'},\n",
        "            {'æ¨™é¡Œ': 'é´»æµ·é›»å‹•è»Šä½ˆå±€åŠ é€Ÿ è¨‚å–®èƒ½è¦‹åº¦æå‡', 'æ‘˜è¦': 'é´»æµ·åœ¨é›»å‹•è»Šé ˜åŸŸæŒçºŒæ“´å±•ï¼Œç²å¾—å¤šç­†è¨‚å–®', 'é€£çµ': 'https://example.com/6', 'ç™¼å¸ƒæ™‚é–“': '2025-10-25 11:00', 'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'ä¾†æº': 'ç¤ºç¯„è³‡æ–™'},\n",
        "            {'æ¨™é¡Œ': 'é‡‘èè‚¡é…æ¯é¡Œæç™¼é…µ å¸å¼•å­˜è‚¡æ—', 'æ‘˜è¦': 'é‡‘èè‚¡é€²å…¥é™¤æ¬Šæ¯æ—ºå­£ï¼Œæ®–åˆ©ç‡å¸å¼•äºº', 'é€£çµ': 'https://example.com/7', 'ç™¼å¸ƒæ™‚é–“': '2025-10-24 15:45', 'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'ä¾†æº': 'ç¤ºç¯„è³‡æ–™'},\n",
        "            {'æ¨™é¡Œ': 'å°è‚¡åŸºé‡‘å–®é€±å¸é‡‘ç ´ç™¾å„„ æŠ•è³‡äººæ¶é€²', 'æ‘˜è¦': 'å°è‚¡åŸºé‡‘å—åˆ°æŠ•è³‡äººé’çï¼Œè³‡é‡‘å¤§é‡æ¹§å…¥', 'é€£çµ': 'https://example.com/8', 'ç™¼å¸ƒæ™‚é–“': '2025-10-24 13:20', 'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'ä¾†æº': 'ç¤ºç¯„è³‡æ–™'},\n",
        "            {'æ¨™é¡Œ': 'èˆªé‹è‚¡å—æ²¹åƒ¹å½±éŸ¿ è‚¡åƒ¹éœ‡ç›ª', 'æ‘˜è¦': 'åœ‹éš›æ²¹åƒ¹æ³¢å‹•ï¼Œå½±éŸ¿èˆªé‹æ¥­æˆæœ¬', 'é€£çµ': 'https://example.com/9', 'ç™¼å¸ƒæ™‚é–“': '2025-10-24 10:10', 'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'ä¾†æº': 'ç¤ºç¯„è³‡æ–™'},\n",
        "            {'æ¨™é¡Œ': 'ç”ŸæŠ€è‚¡ç²æ”¿åºœè£œåŠ© é¡Œæç™¼é…µ', 'æ‘˜è¦': 'æ”¿åºœåŠ ç¢¼ç”ŸæŠ€ç”¢æ¥­è£œåŠ©ï¼Œç›¸é—œå€‹è‚¡å—æƒ ', 'é€£çµ': 'https://example.com/10', 'ç™¼å¸ƒæ™‚é–“': '2025-10-23 16:50', 'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'ä¾†æº': 'ç¤ºç¯„è³‡æ–™'},\n",
        "            {'æ¨™é¡Œ': 'å°å¹£å‡å€¼å£“åŠ› å‡ºå£ç”¢æ¥­å—é—œæ³¨', 'æ‘˜è¦': 'æ–°å°å¹£åŒ¯ç‡èµ°å¼·ï¼Œå‡ºå£ä¼æ¥­é¢è‡¨æŒ‘æˆ°', 'é€£çµ': 'https://example.com/11', 'ç™¼å¸ƒæ™‚é–“': '2025-10-23 14:25', 'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'ä¾†æº': 'ç¤ºç¯„è³‡æ–™'},\n",
        "            {'æ¨™é¡Œ': 'é›»å‹•è»Šä¾›æ‡‰éˆå¤¯ ç›¸é—œå€‹è‚¡æ¼²å‹¢å‡Œå²', 'æ‘˜è¦': 'é›»å‹•è»Šç”¢æ¥­éˆæŒçºŒæˆé•·ï¼Œå¸¶å‹•è‚¡åƒ¹', 'é€£çµ': 'https://example.com/12', 'ç™¼å¸ƒæ™‚é–“': '2025-10-23 11:30', 'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'ä¾†æº': 'ç¤ºç¯„è³‡æ–™'},\n",
        "            {'æ¨™é¡Œ': 'è¨˜æ†¶é«”åƒ¹æ ¼å›å‡ ç›¸é—œå» å•†ç‡Ÿæ”¶çœ‹å¢', 'æ‘˜è¦': 'DRAMèˆ‡NANDåƒ¹æ ¼ä¸Šæ¼²ï¼Œè¨˜æ†¶é«”å» å—æƒ ', 'é€£çµ': 'https://example.com/13', 'ç™¼å¸ƒæ™‚é–“': '2025-10-22 15:15', 'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'ä¾†æº': 'ç¤ºç¯„è³‡æ–™'},\n",
        "            {'æ¨™é¡Œ': 'è¢«å‹•å…ƒä»¶ç¼ºè²¨ å» å•†èª¿æ¼²å ±åƒ¹', 'æ‘˜è¦': 'è¢«å‹•å…ƒä»¶ä¾›ä¸æ‡‰æ±‚ï¼Œåƒ¹æ ¼æŒçºŒä¸Šæ¼²', 'é€£çµ': 'https://example.com/14', 'ç™¼å¸ƒæ™‚é–“': '2025-10-22 12:40', 'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'ä¾†æº': 'ç¤ºç¯„è³‡æ–™'},\n",
        "            {'æ¨™é¡Œ': '5GåŸºå»ºæŠ•è³‡åŠ é€Ÿ é›»ä¿¡è‚¡å—çŸšç›®', 'æ‘˜è¦': 'å„åœ‹åŠ é€Ÿ5Gå»ºè¨­ï¼Œé›»ä¿¡æ¥­è¿ä¾†å•†æ©Ÿ', 'é€£çµ': 'https://example.com/15', 'ç™¼å¸ƒæ™‚é–“': '2025-10-22 09:55', 'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'ä¾†æº': 'ç¤ºç¯„è³‡æ–™'},\n",
        "            {'æ¨™é¡Œ': 'ç¶ èƒ½æ”¿ç­–æ¨å‹• å¤ªé™½èƒ½æ¦‚å¿µè‚¡ç™¼å…‰', 'æ‘˜è¦': 'æ”¿åºœç¶ èƒ½æ”¿ç­–æ˜ç¢ºï¼Œå¤ªé™½èƒ½ç”¢æ¥­å—æƒ ', 'é€£çµ': 'https://example.com/16', 'ç™¼å¸ƒæ™‚é–“': '2025-10-21 16:30', 'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'ä¾†æº': 'ç¤ºç¯„è³‡æ–™'},\n",
        "            {'æ¨™é¡Œ': 'æ™¶åœ“ä»£å·¥ç”¢èƒ½æ»¿è¼‰ æ¥­è€…æ“´å» åŠ é€Ÿ', 'æ‘˜è¦': 'æ™¶åœ“ä»£å·¥éœ€æ±‚æ—ºç››ï¼Œå» å•†ç©æ¥µæ“´ç”¢', 'é€£çµ': 'https://example.com/17', 'ç™¼å¸ƒæ™‚é–“': '2025-10-21 13:45', 'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'ä¾†æº': 'ç¤ºç¯„è³‡æ–™'},\n",
        "            {'æ¨™é¡Œ': 'PCBç”¢æ¥­éœ€æ±‚å›æº« æ³•äººå–Šè²·', 'æ‘˜è¦': 'å°åˆ·é›»è·¯æ¿éœ€æ±‚å¾©ç”¦ï¼Œæ³•äººçœ‹å¥½å¾Œå¸‚', 'é€£çµ': 'https://example.com/18', 'ç™¼å¸ƒæ™‚é–“': '2025-10-21 10:20', 'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'ä¾†æº': 'ç¤ºç¯„è³‡æ–™'},\n",
        "            {'æ¨™é¡Œ': 'å°è‚¡æ¬Šå€¼è‚¡è¼ªå‹• ä¸­å°å‹è‚¡æ´»èº', 'æ‘˜è¦': 'å¤§ç›¤è¼ªå‹•æ•ˆæ‡‰æ˜é¡¯ï¼Œä¸­å°å‹è‚¡äº¤æ˜“ç†±çµ¡', 'é€£çµ': 'https://example.com/19', 'ç™¼å¸ƒæ™‚é–“': '2025-10-20 15:10', 'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'ä¾†æº': 'ç¤ºç¯„è³‡æ–™'},\n",
        "            {'æ¨™é¡Œ': 'ç§‘æŠ€æ¥­äººæ‰è’ å„å…¬å¸åŠ è–ªæ¶äºº', 'æ‘˜è¦': 'ç§‘æŠ€ç”¢æ¥­ç¼ºå·¥åš´é‡ï¼Œä¼æ¥­æé«˜è–ªè³‡', 'é€£çµ': 'https://example.com/20', 'ç™¼å¸ƒæ™‚é–“': '2025-10-20 11:55', 'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'ä¾†æº': 'ç¤ºç¯„è³‡æ–™'},\n",
        "        ]\n",
        "\n",
        "        print(f\"âœ… è¼‰å…¥ {len(self.news_data)} å‰‡ç¤ºç¯„æ–°è\")\n",
        "        return self.news_data\n",
        "\n",
        "    def auto_crawl(self, max_news=30):\n",
        "        \"\"\"è‡ªå‹•é¸æ“‡æœ€ä½³çˆ¬èŸ²æ–¹æ³•\"\"\"\n",
        "        print(\"ğŸš€ é–‹å§‹æ™ºæ…§çˆ¬èŸ²...\")\n",
        "\n",
        "        # å„ªå…ˆé †åºï¼šRSS > å·¥å•†æ™‚å ± > ç¤ºç¯„è³‡æ–™\n",
        "        methods = [\n",
        "            ('RSS Feed', self.crawl_udn_rss),\n",
        "            ('å·¥å•†æ™‚å ±', self.crawl_ctee_news),\n",
        "            ('ç¤ºç¯„è³‡æ–™', self.crawl_demo_data)\n",
        "        ]\n",
        "\n",
        "        for method_name, method in methods:\n",
        "            try:\n",
        "                print(f\"ğŸ“¡ å˜—è©¦ä½¿ç”¨: {method_name}\")\n",
        "                if method_name == 'ç¤ºç¯„è³‡æ–™':\n",
        "                    news = method()\n",
        "                else:\n",
        "                    news = method(max_news)\n",
        "\n",
        "                if news and len(news) > 0:\n",
        "                    print(f\"âœ… æˆåŠŸï¼ä½¿ç”¨ {method_name} å–å¾— {len(news)} å‰‡æ–°è\")\n",
        "                    return news\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ {method_name} å¤±æ•—ï¼Œå˜—è©¦ä¸‹ä¸€å€‹æ–¹æ³•...\")\n",
        "                continue\n",
        "\n",
        "        print(\"âŒ æ‰€æœ‰æ–¹æ³•éƒ½å¤±æ•—\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "Y8VI3TKBQzuo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==================== è²¡ç¶“æ–°èçˆ¬èŸ² ====================\n",
        "class FinanceNewsCrawler:\n",
        "    \"\"\"çˆ¬å–å°è‚¡è²¡ç¶“æ–°è\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "        self.news_data = []\n",
        "\n",
        "    def crawl_cnyes_news(self, pages=3):\n",
        "        \"\"\"çˆ¬å–é‰…äº¨ç¶²å°è‚¡æ–°è\"\"\"\n",
        "        self.news_data = []\n",
        "        base_url = \"https://news.cnyes.com/news/cat/tw_stock\"\n",
        "\n",
        "        for page in range(1, pages + 1):\n",
        "            try:\n",
        "                print(f\"ğŸ” æ­£åœ¨çˆ¬å–ç¬¬ {page}/{pages} é ...\")\n",
        "                url = f\"{base_url}?page={page}\"\n",
        "                response = requests.get(url, headers=self.headers, timeout=10)\n",
        "                response.encoding = 'utf-8'\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                # æ‰¾åˆ°æ–°èåˆ—è¡¨\n",
        "                news_items = soup.find_all('div', class_='_1Zdp')\n",
        "\n",
        "                for item in news_items:\n",
        "                    try:\n",
        "                        # æ¨™é¡Œ\n",
        "                        title_tag = item.find('a')\n",
        "                        if not title_tag:\n",
        "                            continue\n",
        "\n",
        "                        title = title_tag.get_text(strip=True)\n",
        "                        link = \"https://news.cnyes.com\" + title_tag['href']\n",
        "\n",
        "                        # æ™‚é–“\n",
        "                        time_tag = item.find('time')\n",
        "                        publish_time = time_tag.get_text(strip=True) if time_tag else \"æœªçŸ¥æ™‚é–“\"\n",
        "\n",
        "                        # æ‘˜è¦\n",
        "                        summary_tag = item.find('p')\n",
        "                        summary = summary_tag.get_text(strip=True) if summary_tag else \"\"\n",
        "\n",
        "                        self.news_data.append({\n",
        "                            'æ¨™é¡Œ': title,\n",
        "                            'æ‘˜è¦': summary,\n",
        "                            'é€£çµ': link,\n",
        "                            'ç™¼å¸ƒæ™‚é–“': publish_time,\n",
        "                            'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                        })\n",
        "                    except Exception as e:\n",
        "                        continue\n",
        "\n",
        "                time.sleep(2)  # é¿å…è«‹æ±‚å¤ªå¿«\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ ç¬¬ {page} é çˆ¬å–å¤±æ•—: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"âœ… æˆåŠŸçˆ¬å– {len(self.news_data)} å‰‡æ–°è\")\n",
        "        return self.news_data\n",
        "\n",
        "    def crawl_yahoo_finance_news(self, pages=3):\n",
        "        \"\"\"çˆ¬å– Yahoo è²¡ç¶“å°è‚¡æ–°è\"\"\"\n",
        "        self.news_data = []\n",
        "\n",
        "        for page in range(pages):\n",
        "            try:\n",
        "                print(f\"ğŸ” æ­£åœ¨çˆ¬å–ç¬¬ {page+1}/{pages} é ...\")\n",
        "                url = f\"https://tw.stock.yahoo.com/news/list?category=%E5%8F%B0%E8%82%A1&offset={page*10}\"\n",
        "                response = requests.get(url, headers=self.headers, timeout=10)\n",
        "                response.encoding = 'utf-8'\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                # æ‰¾åˆ°æ–°èåˆ—è¡¨\n",
        "                news_items = soup.find_all('div', class_='Ov(h)')\n",
        "\n",
        "                for item in news_items:\n",
        "                    try:\n",
        "                        title_tag = item.find('a')\n",
        "                        if not title_tag:\n",
        "                            continue\n",
        "\n",
        "                        title = title_tag.get_text(strip=True)\n",
        "                        link = \"https://tw.stock.yahoo.com\" + title_tag['href']\n",
        "\n",
        "                        # æ™‚é–“\n",
        "                        time_tag = item.find('time')\n",
        "                        publish_time = time_tag.get_text(strip=True) if time_tag else \"æœªçŸ¥æ™‚é–“\"\n",
        "\n",
        "                        self.news_data.append({\n",
        "                            'æ¨™é¡Œ': title,\n",
        "                            'æ‘˜è¦': '',\n",
        "                            'é€£çµ': link,\n",
        "                            'ç™¼å¸ƒæ™‚é–“': publish_time,\n",
        "                            'çˆ¬å–æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                        })\n",
        "                    except Exception as e:\n",
        "                        continue\n",
        "\n",
        "                time.sleep(2)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ ç¬¬ {page+1} é çˆ¬å–å¤±æ•—: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"âœ… æˆåŠŸçˆ¬å– {len(self.news_data)} å‰‡æ–°è\")\n",
        "        return self.news_data"
      ],
      "metadata": {
        "id": "o3sKVIGVM8-m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== æ–‡å­—åˆ†æï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼‰====================\n",
        "class TextAnalyzer:\n",
        "    \"\"\"æ–‡å­—åˆ†æèˆ‡é—œéµå­—æå– - çµå·´åˆ†è©å¢å¼·ç‰ˆ\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"ğŸ”§ åˆå§‹åŒ–çµå·´åˆ†è©...\")\n",
        "\n",
        "        # è¼‰å…¥çµå·´åˆ†è©\n",
        "        import jieba\n",
        "        import jieba.analyse\n",
        "\n",
        "        # æ“´å……åœç”¨è©åˆ—è¡¨ï¼ˆåŒ…å«ç¶²å€ç›¸é—œè©ï¼‰\n",
        "        self.stopwords = set([\n",
        "            # åŸºæœ¬åœç”¨è©\n",
        "            'çš„', 'æ˜¯', 'åœ¨', 'äº†', 'å’Œ', 'èˆ‡', 'ç­‰', 'å°‡', 'å¯', 'æˆ–',\n",
        "            'ä½†', 'å°', 'ç‚º', 'åŠ', 'ä»¥', 'æ–¼', 'å¾', 'æ›´', 'å¾ˆ', 'æœ€',\n",
        "            'ä¹Ÿ', 'éƒ½', 'å°±', 'åˆ°', 'è¢«', 'æœ‰', 'é€™', 'é‚£', 'å€‹', 'ä»–',\n",
        "            'å¥¹', 'æˆ‘', 'ä½ ', 'å€‘', 'ä¸­', 'ä¸Š', 'ä¸‹', 'ä¾†', 'å»', 'èªª',\n",
        "            'è¦', 'æœƒ', 'èƒ½', 'æŠŠ', 'è®“', 'çµ¦', 'æ²’', 'åª', 'é‚„', 'åˆ',\n",
        "            'å‰‡', 'ç¯‡', 'ä½', 'å®¶', 'é–“', 'å', 'æ¬¡', 'é …', 'ä»¶', 'å¼µ',\n",
        "            'ä¸', 'å…¶', 'è€Œ', 'å› ', 'æ‰€', 'å‰‡', 'å¾—', 'è‘—', 'é', 'ä¾¿',\n",
        "            'ç”¨', 'ä¸¦', 'å‘', 'å¦‚', 'ä¸”', 'å†', 'å¦', 'å³', 'æˆ–è¨±', 'æ­¤',\n",
        "            'è¡¨ç¤º', 'æŒ‡å‡º', 'èªç‚º', 'æ ¹æ“š', 'é€é', 'ç›®å‰', 'ä»Šå¹´', 'å»å¹´',\n",
        "            # ç¶²å€ç›¸é—œåœç”¨è©ï¼ˆé—œéµï¼ï¼‰\n",
        "            'http', 'https', 'www', 'com', 'tw', 'net', 'org', 'url',\n",
        "            'amp', 'photo', 'image', 'jpg', 'png', 'html', 'php',\n",
        "            'udn', 'news', 'article', 'id', 'link', 'href',\n",
        "            # æ•¸å­—ç›¸é—œ\n",
        "            '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "            '10', '20', '30', '100', '180', '360', '720', '1080',\n",
        "            '3600', '2024', '2025', '2026',\n",
        "            # æ™‚é–“ç›¸é—œé€šç”¨è©\n",
        "            'ä»Šå¤©', 'æ˜¨å¤©', 'æ˜å¤©', 'ä¸Šåˆ', 'ä¸‹åˆ', 'æ™šä¸Š'\n",
        "        ])\n",
        "\n",
        "        # è‡ªè¨‚è©å…¸ï¼ˆé‡‘èç›¸é—œè©å½™ï¼‰\n",
        "        financial_terms = [\n",
        "            # å…¬å¸åç¨±\n",
        "            'å°ç©é›»', 'è¯ç™¼ç§‘', 'é´»æµ·', 'å¤§ç«‹å…‰', 'å°é”é›»', 'è¯é›»',\n",
        "            'æ—¥æœˆå…‰', 'åœ‹å·¨', 'è¯ç¢©', 'å»£é”', 'ä»å¯¶', 'å’Œç¢©',\n",
        "            'å°å¡‘', 'ä¸­é‹¼', 'å¯Œé‚¦é‡‘', 'åœ‹æ³°é‡‘', 'å…†è±é‡‘', 'ç‰å±±é‡‘',\n",
        "            # é‡‘èè¡“èª\n",
        "            'å¤–è³‡', 'æŠ•ä¿¡', 'è‡ªç‡Ÿå•†', 'ä¸‰å¤§æ³•äºº', 'æ•£æˆ¶',\n",
        "            'æ¼²åœ', 'è·Œåœ', 'æ¼²è·Œå¹…', 'æˆäº¤é‡', 'èè³‡', 'èåˆ¸',\n",
        "            'æœ¬ç›Šæ¯”', 'æ®–åˆ©ç‡', 'è‚¡æ¯', 'é…æ¯', 'é™¤æ¬Šæ¯',\n",
        "            'ç¾é‡‘è‚¡åˆ©', 'è‚¡ç¥¨è‚¡åˆ©', 'å¡«æ¬Šæ¯',\n",
        "            # ç”¢æ¥­é¡åˆ¥\n",
        "            'åŠå°é«”', 'é›»å­è‚¡', 'é‡‘èè‚¡', 'å‚³ç”¢è‚¡', 'ç”ŸæŠ€è‚¡',\n",
        "            'èˆªé‹è‚¡', 'é‹¼éµè‚¡', 'å¡‘åŒ–è‚¡', 'ç‡Ÿå»ºè‚¡', 'è§€å…‰è‚¡',\n",
        "            # è²¡å‹™æŒ‡æ¨™\n",
        "            'ç‡Ÿæ”¶', 'ç²åˆ©', 'è²¡å ±', 'EPS', 'ROE', 'ROA',\n",
        "            'æ¯›åˆ©ç‡', 'ç‡Ÿç›Šç‡', 'æ·¨åˆ©ç‡', 'è² å‚µæ¯”',\n",
        "            # å¸‚å ´ç›¸é—œ\n",
        "            'ç¾è‚¡', 'å°è‚¡', 'é™¸è‚¡', 'æ¸¯è‚¡', 'æ—¥è‚¡',\n",
        "            'åŠ æ¬ŠæŒ‡æ•¸', 'æ«ƒè²·æŒ‡æ•¸', 'é“ç“Š', 'é‚£æ–¯é”å…‹', 'S&P',\n",
        "            'å¤šé ­', 'ç©ºé ­', 'ç›¤æ•´', 'çªç ´', 'æ”¯æ’', 'å£“åŠ›',\n",
        "            # ç¸½ç¶“ç›¸é—œ\n",
        "            'å‡æ¯', 'é™æ¯', 'é€šè†¨', 'é€šç¸®', 'ç¶“æ¿Ÿæˆé•·', 'GDP',\n",
        "            'å¤®è¡Œ', 'è¯æº–æœƒ', 'è²¨å¹£æ”¿ç­–', 'è²¡æ”¿æ”¿ç­–',\n",
        "            # ç§‘æŠ€è¶¨å‹¢\n",
        "            'AI', 'äººå·¥æ™ºæ…§', 'é›»å‹•è»Š', '5G', 'ç‰©è¯ç¶²',\n",
        "            'å…ƒå®‡å®™', 'å€å¡Šéˆ', 'é›²ç«¯', 'å¤§æ•¸æ“š',\n",
        "            # å…¶ä»–\n",
        "            'æŠ•è³‡', 'è²·é€²', 'è³£å‡º', 'æŒæœ‰', 'å¸ƒå±€', 'ç²åˆ©äº†çµ'\n",
        "        ]\n",
        "\n",
        "        # å°‡è‡ªè¨‚è©åŠ å…¥çµå·´è©å…¸\n",
        "        for term in financial_terms:\n",
        "            jieba.add_word(term, freq=10000)  # è¨­å®šé«˜é »ç‡ç¢ºä¿è¢«è­˜åˆ¥\n",
        "\n",
        "        print(\"âœ… çµå·´åˆ†è©åˆå§‹åŒ–å®Œæˆ\")\n",
        "        print(f\"ğŸ“š åœç”¨è©æ•¸é‡: {len(self.stopwords)} å€‹\")\n",
        "        print(f\"ğŸ“– è‡ªè¨‚è©å½™æ•¸é‡: {len(financial_terms)} å€‹\")\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"æ–‡å­—é è™•ç† - æ¸…é™¤ç¶²å€å’Œé›œè¨Š\"\"\"\n",
        "        # 1. ç§»é™¤ç¶²å€\n",
        "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "        # 2. ç§»é™¤ email\n",
        "        text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "        # 3. ç§»é™¤æ•¸å­—ï¼ˆä½†ä¿ç•™ä¸­æ–‡æ•¸å­—ï¼‰\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "        # 4. ç§»é™¤ç‰¹æ®Šç¬¦è™Ÿï¼ˆä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ï¼‰\n",
        "        text = re.sub(r'[^\\w\\s\\u4e00-\\u9fff]', ' ', text)\n",
        "\n",
        "        # 5. ç§»é™¤å¤šé¤˜ç©ºç™½\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        return text\n",
        "\n",
        "    def is_valid_keyword(self, word):\n",
        "        \"\"\"åˆ¤æ–·æ˜¯å¦ç‚ºæœ‰æ•ˆé—œéµå­—\"\"\"\n",
        "        # æª¢æŸ¥é•·åº¦\n",
        "        if len(word) < 2:\n",
        "            return False\n",
        "\n",
        "        # æª¢æŸ¥æ˜¯å¦ç‚ºåœç”¨è©\n",
        "        if word.lower() in self.stopwords:\n",
        "            return False\n",
        "\n",
        "        # æª¢æŸ¥æ˜¯å¦å…¨ç‚ºæ•¸å­—\n",
        "        if word.isdigit():\n",
        "            return False\n",
        "\n",
        "        # æª¢æŸ¥æ˜¯å¦ç‚ºç´”è‹±æ–‡ä¸”å¤ªçŸ­\n",
        "        if word.isalpha() and len(word) < 3:\n",
        "            return False\n",
        "\n",
        "        # æª¢æŸ¥æ˜¯å¦åŒ…å«ç¶²å€é—œéµå­—\n",
        "        url_keywords = ['http', 'www', 'com', 'tw', 'net', 'html', 'php', 'amp']\n",
        "        if any(uk in word.lower() for uk in url_keywords):\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def extract_keywords_jieba(self, texts, top_n=10):\n",
        "        \"\"\"ä½¿ç”¨ jieba.analyse æå–é—œéµå­—\"\"\"\n",
        "        print(\"ğŸ” ä½¿ç”¨ jieba.analyse æå–é—œéµå­—...\")\n",
        "\n",
        "        # é è™•ç†ä¸¦åˆä½µæ‰€æœ‰æ–‡å­—\n",
        "        processed_texts = [self.preprocess_text(text) for text in texts]\n",
        "        combined_text = ' '.join(processed_texts)\n",
        "\n",
        "        # ä½¿ç”¨ jieba çš„ TF-IDF æå–é—œéµå­—\n",
        "        keywords = jieba.analyse.extract_tags(\n",
        "            combined_text,\n",
        "            topK=top_n * 3,  # æå–æ›´å¤šï¼Œä¹‹å¾Œå†éæ¿¾\n",
        "            withWeight=True,\n",
        "            allowPOS=('n', 'nr', 'ns', 'nt', 'nz', 'v', 'vn', 'a', 'an')\n",
        "        )\n",
        "\n",
        "        # éæ¿¾ç„¡æ•ˆé—œéµå­—\n",
        "        valid_keywords = [\n",
        "            (word, weight) for word, weight in keywords\n",
        "            if self.is_valid_keyword(word)\n",
        "        ]\n",
        "\n",
        "        print(f\"âœ… æå–äº† {len(valid_keywords[:top_n])} å€‹æœ‰æ•ˆé—œéµå­—\")\n",
        "        return valid_keywords[:top_n]\n",
        "\n",
        "    def extract_keywords_textrank(self, texts, top_n=10):\n",
        "        \"\"\"ä½¿ç”¨ TextRank æ¼”ç®—æ³•æå–é—œéµå­—\"\"\"\n",
        "        print(\"ğŸ” ä½¿ç”¨ TextRank æ¼”ç®—æ³•æå–é—œéµå­—...\")\n",
        "\n",
        "        # é è™•ç†ä¸¦åˆä½µæ‰€æœ‰æ–‡å­—\n",
        "        processed_texts = [self.preprocess_text(text) for text in texts]\n",
        "        combined_text = ' '.join(processed_texts)\n",
        "\n",
        "        # ä½¿ç”¨ jieba çš„ TextRank æå–é—œéµå­—\n",
        "        keywords = jieba.analyse.textrank(\n",
        "            combined_text,\n",
        "            topK=top_n * 3,\n",
        "            withWeight=True,\n",
        "            allowPOS=('n', 'nr', 'ns', 'nt', 'nz', 'v', 'vn', 'a', 'an')\n",
        "        )\n",
        "\n",
        "        # éæ¿¾ç„¡æ•ˆé—œéµå­—\n",
        "        valid_keywords = [\n",
        "            (word, weight) for word, weight in keywords\n",
        "            if self.is_valid_keyword(word)\n",
        "        ]\n",
        "\n",
        "        print(f\"âœ… æå–äº† {len(valid_keywords[:top_n])} å€‹æœ‰æ•ˆé—œéµå­—\")\n",
        "        return valid_keywords[:top_n]\n",
        "\n",
        "    def word_frequency(self, texts, top_n=10):\n",
        "        \"\"\"è©é »çµ±è¨ˆ\"\"\"\n",
        "        print(\"ğŸ” é€²è¡Œè©é »çµ±è¨ˆ...\")\n",
        "\n",
        "        # é è™•ç†ä¸¦åˆä½µæ‰€æœ‰æ–‡å­—\n",
        "        processed_texts = [self.preprocess_text(text) for text in texts]\n",
        "        combined_text = ' '.join(processed_texts)\n",
        "\n",
        "        # ä½¿ç”¨çµå·´åˆ†è©\n",
        "        words = jieba.cut(combined_text)\n",
        "\n",
        "        # éæ¿¾ä¸¦çµ±è¨ˆ\n",
        "        valid_words = [\n",
        "            w.strip() for w in words\n",
        "            if w.strip() and self.is_valid_keyword(w.strip())\n",
        "        ]\n",
        "\n",
        "        # è¨ˆç®—è©é »\n",
        "        word_counts = Counter(valid_words)\n",
        "\n",
        "        result = word_counts.most_common(top_n)\n",
        "        print(f\"âœ… çµ±è¨ˆäº† {len(word_counts)} å€‹ä¸åŒçš„è©ï¼Œè¿”å›å‰ {top_n} å€‹\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def tfidf_analysis(self, texts, top_n=10):\n",
        "        \"\"\"ä½¿ç”¨ sklearn TF-IDF åˆ†æ\"\"\"\n",
        "        print(\"ğŸ” ä½¿ç”¨ TF-IDF åˆ†æé—œéµå­—...\")\n",
        "\n",
        "        # ä¸­æ–‡åˆ†è©å‡½æ•¸\n",
        "        def tokenize(text):\n",
        "            text = self.preprocess_text(text)\n",
        "            words = jieba.cut(text)\n",
        "            valid = [w.strip() for w in words if w.strip() and self.is_valid_keyword(w.strip())]\n",
        "            return ' '.join(valid)\n",
        "\n",
        "        # å°æ‰€æœ‰æ–‡å­—é€²è¡Œåˆ†è©\n",
        "        tokenized_texts = [tokenize(text) for text in texts]\n",
        "\n",
        "        # éæ¿¾ç©ºæ–‡æœ¬\n",
        "        tokenized_texts = [t for t in tokenized_texts if t.strip()]\n",
        "\n",
        "        if not tokenized_texts:\n",
        "            print(\"âš ï¸ æ²’æœ‰æœ‰æ•ˆæ–‡æœ¬ï¼Œæ”¹ç”¨ jieba æ–¹æ³•\")\n",
        "            return self.extract_keywords_jieba(texts, top_n)\n",
        "\n",
        "        try:\n",
        "            # å»ºç«‹ TF-IDF å‘é‡åŒ–å™¨\n",
        "            vectorizer = TfidfVectorizer(\n",
        "                max_features=top_n * 3,\n",
        "                token_pattern=r'(?u)\\b\\w\\w+\\b',  # è‡³å°‘2å€‹å­—ç¬¦\n",
        "                min_df=1,  # è‡³å°‘å‡ºç¾åœ¨1å€‹æ–‡æª”ä¸­\n",
        "                max_df=0.8  # æœ€å¤šå‡ºç¾åœ¨80%çš„æ–‡æª”ä¸­\n",
        "            )\n",
        "\n",
        "            # è¨ˆç®— TF-IDF çŸ©é™£\n",
        "            tfidf_matrix = vectorizer.fit_transform(tokenized_texts)\n",
        "\n",
        "            # å–å¾—ç‰¹å¾µåç¨±\n",
        "            feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "            # è¨ˆç®—æ¯å€‹é—œéµå­—çš„ç¸½åˆ†æ•¸\n",
        "            tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
        "\n",
        "            # å»ºç«‹é…å°\n",
        "            keywords = list(zip(feature_names, tfidf_scores))\n",
        "\n",
        "            # å†æ¬¡éæ¿¾ï¼ˆé›™é‡ä¿éšªï¼‰\n",
        "            keywords = [(w, s) for w, s in keywords if self.is_valid_keyword(w)]\n",
        "\n",
        "            # æ’åº\n",
        "            keywords.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            print(f\"âœ… TF-IDF åˆ†æå®Œæˆï¼Œæå– {min(top_n, len(keywords))} å€‹é—œéµå­—\")\n",
        "\n",
        "            return keywords[:top_n]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ TF-IDF åˆ†æå¤±æ•—: {str(e)}\")\n",
        "            print(\"ğŸ”„ æ”¹ç”¨ jieba é—œéµå­—æå–...\")\n",
        "            return self.extract_keywords_jieba(texts, top_n)\n",
        "\n",
        "    def comprehensive_analysis(self, texts, top_n=10):\n",
        "        \"\"\"ç¶œåˆåˆ†æ\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ğŸš€ é–‹å§‹ç¶œåˆæ–‡å­—åˆ†æï¼ˆå·²éæ¿¾ç¶²å€å’Œé›œè¨Šï¼‰\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        results = {\n",
        "            'word_frequency': [],\n",
        "            'jieba_tfidf': [],\n",
        "            'textrank': [],\n",
        "            'sklearn_tfidf': []\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            print(\"\\nğŸ“Š æ–¹æ³•1: è©é »çµ±è¨ˆ\")\n",
        "            results['word_frequency'] = self.word_frequency(texts, top_n)\n",
        "            self._print_keywords(results['word_frequency'], 'è©é »çµ±è¨ˆ')\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ è©é »çµ±è¨ˆå¤±æ•—: {str(e)}\")\n",
        "\n",
        "        try:\n",
        "            print(\"\\nğŸ“Š æ–¹æ³•2: jieba TF-IDF\")\n",
        "            results['jieba_tfidf'] = self.extract_keywords_jieba(texts, top_n)\n",
        "            self._print_keywords(results['jieba_tfidf'], 'jieba TF-IDF')\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ jieba TF-IDF å¤±æ•—: {str(e)}\")\n",
        "\n",
        "        try:\n",
        "            print(\"\\nğŸ“Š æ–¹æ³•3: TextRank æ¼”ç®—æ³•\")\n",
        "            results['textrank'] = self.extract_keywords_textrank(texts, top_n)\n",
        "            self._print_keywords(results['textrank'], 'TextRank')\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ TextRank å¤±æ•—: {str(e)}\")\n",
        "\n",
        "        try:\n",
        "            print(\"\\nğŸ“Š æ–¹æ³•4: sklearn TF-IDF\")\n",
        "            results['sklearn_tfidf'] = self.tfidf_analysis(texts, top_n)\n",
        "            self._print_keywords(results['sklearn_tfidf'], 'sklearn TF-IDF')\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ sklearn TF-IDF å¤±æ•—: {str(e)}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"âœ… ç¶œåˆåˆ†æå®Œæˆ\")\n",
        "        print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _print_keywords(self, keywords, method_name):\n",
        "        \"\"\"è¼”åŠ©å‡½æ•¸ï¼šå°å‡ºé—œéµå­—\"\"\"\n",
        "        if keywords:\n",
        "            print(f\"\\nã€{method_name}ã€‘å‰ 10 å€‹é—œéµå­—:\")\n",
        "            for idx, (word, score) in enumerate(keywords[:10], 1):\n",
        "                print(f\"  {idx}. {word} ({score:.4f})\")\n",
        "        else:\n",
        "            print(f\"ã€{method_name}ã€‘ç„¡çµæœ\")\n",
        "\n",
        "    def get_best_keywords(self, texts, top_n=10, method='tfidf'):\n",
        "        \"\"\"å–å¾—æœ€ä½³é—œéµå­—\"\"\"\n",
        "        if method == 'tfidf':\n",
        "            return self.tfidf_analysis(texts, top_n)\n",
        "        elif method == 'textrank':\n",
        "            return self.extract_keywords_textrank(texts, top_n)\n",
        "        elif method == 'frequency':\n",
        "            return self.word_frequency(texts, top_n)\n",
        "        elif method == 'comprehensive':\n",
        "            # ç¶œåˆå¤šç¨®æ–¹æ³•\n",
        "            jieba_kw = dict(self.extract_keywords_jieba(texts, top_n * 2))\n",
        "            textrank_kw = dict(self.extract_keywords_textrank(texts, top_n * 2))\n",
        "\n",
        "            all_keywords = set(jieba_kw.keys()) | set(textrank_kw.keys())\n",
        "            combined_scores = {}\n",
        "\n",
        "            for kw in all_keywords:\n",
        "                if self.is_valid_keyword(kw):  # å†æ¬¡ç¢ºèª\n",
        "                    score = 0\n",
        "                    if kw in jieba_kw:\n",
        "                        score += jieba_kw[kw]\n",
        "                    if kw in textrank_kw:\n",
        "                        score += textrank_kw[kw]\n",
        "                    combined_scores[kw] = score\n",
        "\n",
        "            sorted_kw = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "            return sorted_kw[:top_n]\n",
        "        else:\n",
        "            return self.tfidf_analysis(texts, top_n)"
      ],
      "metadata": {
        "id": "8u1zPfSRNBdB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== ä¿®æ­£å¾Œçš„è¦–è¦ºåŒ–æ¨¡çµ„ ====================\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "import base64\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class DataVisualizer:\n",
        "    \"\"\"è³‡æ–™è¦–è¦ºåŒ–é¡åˆ¥ - ä½¿ç”¨ Plotly\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.colors = px.colors.qualitative.Set3\n",
        "\n",
        "    def create_wordcloud(self, keywords, title=\"é—œéµå­—æ–‡å­—é›²\", max_words=50):\n",
        "        \"\"\"\n",
        "        ç”Ÿæˆæ–‡å­—é›²åœ– - ä¿®æ­£ç‰ˆï¼ˆè¿”å›PIL Imageï¼‰\n",
        "        keywords: [(è©, åˆ†æ•¸), ...] æ ¼å¼\n",
        "        \"\"\"\n",
        "        print(f\"ğŸ¨ ç”Ÿæˆæ–‡å­—é›²: {title}\")\n",
        "\n",
        "        try:\n",
        "            # æº–å‚™æ–‡å­—é›²è³‡æ–™\n",
        "            word_freq = {word: float(score) for word, score in keywords[:max_words]}\n",
        "\n",
        "            if not word_freq:\n",
        "                print(\"âš ï¸ æ²’æœ‰é—œéµå­—è³‡æ–™\")\n",
        "                return None\n",
        "\n",
        "            # ç”Ÿæˆæ–‡å­—é›²\n",
        "            wordcloud = WordCloud(\n",
        "                width=1600,\n",
        "                height=800,\n",
        "                background_color='white',\n",
        "                font_path='/usr/share/fonts/truetype/wqy/wqy-microhei.ttc',  # ä¸­æ–‡å­—é«”\n",
        "                colormap='viridis',\n",
        "                max_words=max_words,\n",
        "                relative_scaling=0.5,\n",
        "                min_font_size=12,\n",
        "                prefer_horizontal=0.7,\n",
        "                collocations=False  # é¿å…é‡è¤‡è©çµ„\n",
        "            ).generate_from_frequencies(word_freq)\n",
        "\n",
        "            # è½‰æ›ç‚º numpy array\n",
        "            wordcloud_array = wordcloud.to_array()\n",
        "\n",
        "            # è½‰æ›ç‚º PIL Image\n",
        "            wordcloud_image = Image.fromarray(wordcloud_array)\n",
        "\n",
        "            # å„²å­˜ç‚ºè‡¨æ™‚æª”æ¡ˆ\n",
        "            temp_path = '/tmp/wordcloud.png'\n",
        "            wordcloud_image.save(temp_path, format='PNG', dpi=(300, 300))\n",
        "\n",
        "            print(f\"âœ… æ–‡å­—é›²ç”Ÿæˆå®Œæˆ ({len(word_freq)} å€‹è©)\")\n",
        "            print(f\"ğŸ“ å„²å­˜è·¯å¾‘: {temp_path}\")\n",
        "\n",
        "            return temp_path  # è¿”å›æª”æ¡ˆè·¯å¾‘\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ æ–‡å­—é›²ç”Ÿæˆå¤±æ•—: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def create_wordcloud_plotly(self, keywords, title=\"é—œéµå­—æ–‡å­—é›²\", max_words=50):\n",
        "        \"\"\"\n",
        "        ä½¿ç”¨ Plotly è£½ä½œæ–‡å­—é›²æ›¿ä»£æ–¹æ¡ˆï¼ˆæ•£é»åœ–æ¨¡æ“¬ï¼‰\n",
        "        \"\"\"\n",
        "        print(f\"ğŸ¨ ç”Ÿæˆ Plotly æ–‡å­—é›²: {title}\")\n",
        "\n",
        "        try:\n",
        "            top_keywords = keywords[:max_words]\n",
        "\n",
        "            if not top_keywords:\n",
        "                print(\"âš ï¸ æ²’æœ‰é—œéµå­—è³‡æ–™\")\n",
        "                return None\n",
        "\n",
        "            words = [kw for kw, _ in top_keywords]\n",
        "            scores = [float(score) for _, score in top_keywords]\n",
        "\n",
        "            # æ­£è¦åŒ–åˆ†æ•¸\n",
        "            max_score = max(scores)\n",
        "            min_score = min(scores)\n",
        "\n",
        "            # è¨ˆç®—å­—é«”å¤§å°ï¼ˆ10-60ï¼‰\n",
        "            sizes = [10 + (s - min_score) / (max_score - min_score) * 50 for s in scores]\n",
        "\n",
        "            # éš¨æ©Ÿä½†å›ºå®šçš„ä½ç½®\n",
        "            import random\n",
        "            random.seed(42)\n",
        "\n",
        "            # ä½¿ç”¨èºæ—‹ä½ˆå±€\n",
        "            angles = [i * 137.5 for i in range(len(words))]  # é»ƒé‡‘è§’åº¦\n",
        "            radii = [i ** 0.5 for i in range(len(words))]\n",
        "\n",
        "            x_coords = [r * np.cos(np.radians(a)) for r, a in zip(radii, angles)]\n",
        "            y_coords = [r * np.sin(np.radians(a)) for r, a in zip(radii, angles)]\n",
        "\n",
        "            # å»ºç«‹æ•£é»åœ–\n",
        "            fig = go.Figure()\n",
        "\n",
        "            for i, (word, x, y, size, score) in enumerate(zip(words, x_coords, y_coords, sizes, scores)):\n",
        "                fig.add_trace(go.Scatter(\n",
        "                    x=[x],\n",
        "                    y=[y],\n",
        "                    mode='text',\n",
        "                    text=word,\n",
        "                    textfont=dict(\n",
        "                        size=size,\n",
        "                        color=f'hsl({i * 360 / len(words)}, 70%, 50%)'\n",
        "                    ),\n",
        "                    hovertemplate=f'<b>{word}</b><br>åˆ†æ•¸: {score:.4f}<extra></extra>',\n",
        "                    showlegend=False\n",
        "                ))\n",
        "\n",
        "            fig.update_layout(\n",
        "                title=dict(\n",
        "                    text=f'<b>{title}</b>',\n",
        "                    x=0.5,\n",
        "                    xanchor='center',\n",
        "                    font=dict(size=24)\n",
        "                ),\n",
        "                xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),\n",
        "                yaxis=dict(showgrid=False, showticklabels=False, zeroline=False),\n",
        "                height=800,\n",
        "                template='plotly_white',\n",
        "                hovermode='closest',\n",
        "                plot_bgcolor='rgba(240, 240, 240, 0.5)'\n",
        "            )\n",
        "\n",
        "            print(f\"âœ… Plotly æ–‡å­—é›²ç”Ÿæˆå®Œæˆ\")\n",
        "            return fig\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Plotly æ–‡å­—é›²ç”Ÿæˆå¤±æ•—: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def plot_top_keywords_bar(self, keywords, title=\"å‰20é—œéµå­—æ’è¡Œ\", top_n=20):\n",
        "        \"\"\"é—œéµå­—æ©«æ¢åœ–ï¼ˆPlotlyï¼‰\"\"\"\n",
        "        print(f\"ğŸ“Š ç”Ÿæˆé—œéµå­—æ©«æ¢åœ–: {title}\")\n",
        "\n",
        "        try:\n",
        "            top_keywords = keywords[:top_n]\n",
        "\n",
        "            if not top_keywords:\n",
        "                print(\"âš ï¸ æ²’æœ‰é—œéµå­—è³‡æ–™\")\n",
        "                return None\n",
        "\n",
        "            # åè½‰é †åº\n",
        "            words = [kw for kw, _ in reversed(top_keywords)]\n",
        "            scores = [float(score) for _, score in reversed(top_keywords)]\n",
        "\n",
        "            # å»ºç«‹æ©«æ¢åœ–\n",
        "            fig = go.Figure(go.Bar(\n",
        "                x=scores,\n",
        "                y=words,\n",
        "                orientation='h',\n",
        "                marker=dict(\n",
        "                    color=scores,\n",
        "                    colorscale='Viridis',\n",
        "                    showscale=True,\n",
        "                    colorbar=dict(title=\"åˆ†æ•¸\")\n",
        "                ),\n",
        "                text=[f'{s:.4f}' for s in scores],\n",
        "                textposition='outside',\n",
        "                hovertemplate='<b>%{y}</b><br>åˆ†æ•¸: %{x:.4f}<extra></extra>'\n",
        "            ))\n",
        "\n",
        "            fig.update_layout(\n",
        "                title=dict(\n",
        "                    text=f'<b>{title}</b>',\n",
        "                    x=0.5,\n",
        "                    xanchor='center',\n",
        "                    font=dict(size=20)\n",
        "                ),\n",
        "                xaxis_title='TF-IDF åˆ†æ•¸',\n",
        "                yaxis_title='é—œéµå­—',\n",
        "                height=max(500, top_n * 30),\n",
        "                showlegend=False,\n",
        "                template='plotly_white',\n",
        "                margin=dict(l=120, r=50, t=80, b=50)\n",
        "            )\n",
        "\n",
        "            print(f\"âœ… æ©«æ¢åœ–ç”Ÿæˆå®Œæˆ ({len(top_keywords)} å€‹é—œéµå­—)\")\n",
        "            return fig\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ æ©«æ¢åœ–ç”Ÿæˆå¤±æ•—: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    # ... å…¶ä»–è¦–è¦ºåŒ–æ–¹æ³•ä¿æŒä¸è®Š ...\n",
        "\n",
        "    def plot_keyword_distribution_pie(self, keywords, title=\"é—œéµå­—åˆ†æ•¸åˆ†å¸ƒ\", top_n=15):\n",
        "        \"\"\"é—œéµå­—åœ“é¤…åœ–\"\"\"\n",
        "        print(f\"ğŸ“Š ç”Ÿæˆé—œéµå­—åœ“é¤…åœ–: {title}\")\n",
        "\n",
        "        try:\n",
        "            top_keywords = keywords[:top_n]\n",
        "\n",
        "            if not top_keywords:\n",
        "                print(\"âš ï¸ æ²’æœ‰é—œéµå­—è³‡æ–™\")\n",
        "                return None\n",
        "\n",
        "            words = [kw for kw, _ in top_keywords]\n",
        "            scores = [float(score) for _, score in top_keywords]\n",
        "\n",
        "            fig = go.Figure(data=[go.Pie(\n",
        "                labels=words,\n",
        "                values=scores,\n",
        "                hole=0.4,\n",
        "                marker=dict(\n",
        "                    colors=px.colors.qualitative.Set3,\n",
        "                    line=dict(color='white', width=2)\n",
        "                ),\n",
        "                textinfo='label+percent',\n",
        "                hovertemplate='<b>%{label}</b><br>åˆ†æ•¸: %{value:.4f}<br>ä½”æ¯”: %{percent}<extra></extra>'\n",
        "            )])\n",
        "\n",
        "            fig.update_layout(\n",
        "                title=dict(\n",
        "                    text=f'<b>{title}</b>',\n",
        "                    x=0.5,\n",
        "                    xanchor='center',\n",
        "                    font=dict(size=20)\n",
        "                ),\n",
        "                showlegend=True,\n",
        "                height=600,\n",
        "                template='plotly_white',\n",
        "                legend=dict(\n",
        "                    orientation=\"v\",\n",
        "                    yanchor=\"middle\",\n",
        "                    y=0.5,\n",
        "                    xanchor=\"left\",\n",
        "                    x=1.05\n",
        "                )\n",
        "            )\n",
        "\n",
        "            print(f\"âœ… åœ“é¤…åœ–ç”Ÿæˆå®Œæˆ\")\n",
        "            return fig\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ åœ“é¤…åœ–ç”Ÿæˆå¤±æ•—: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def plot_methods_comparison(self, analysis_results, top_n=10):\n",
        "        \"\"\"æ¯”è¼ƒä¸åŒåˆ†ææ–¹æ³•\"\"\"\n",
        "        print(\"ğŸ“Š ç”Ÿæˆåˆ†ææ–¹æ³•æ¯”è¼ƒåœ–\")\n",
        "\n",
        "        try:\n",
        "            methods = {\n",
        "                'word_frequency': 'è©é »çµ±è¨ˆ',\n",
        "                'jieba_tfidf': 'Jieba TF-IDF',\n",
        "                'textrank': 'TextRank',\n",
        "                'sklearn_tfidf': 'Sklearn TF-IDF'\n",
        "            }\n",
        "\n",
        "            all_keywords = set()\n",
        "            for method_key in methods.keys():\n",
        "                if method_key in analysis_results and analysis_results[method_key]:\n",
        "                    for word, _ in analysis_results[method_key][:top_n]:\n",
        "                        all_keywords.add(word)\n",
        "\n",
        "            if not all_keywords:\n",
        "                print(\"âš ï¸ æ²’æœ‰é—œéµå­—è³‡æ–™\")\n",
        "                return None\n",
        "\n",
        "            data = []\n",
        "            for method_key, method_name in methods.items():\n",
        "                if method_key in analysis_results and analysis_results[method_key]:\n",
        "                    keywords_dict = dict(analysis_results[method_key][:top_n])\n",
        "                    for word in all_keywords:\n",
        "                        score = float(keywords_dict.get(word, 0))\n",
        "                        if score > 0:\n",
        "                            data.append({\n",
        "                                'æ–¹æ³•': method_name,\n",
        "                                'é—œéµå­—': word,\n",
        "                                'åˆ†æ•¸': score\n",
        "                            })\n",
        "\n",
        "            if not data:\n",
        "                print(\"âš ï¸ æ²’æœ‰æœ‰æ•ˆè³‡æ–™\")\n",
        "                return None\n",
        "\n",
        "            df = pd.DataFrame(data)\n",
        "\n",
        "            fig = px.bar(\n",
        "                df,\n",
        "                x='åˆ†æ•¸',\n",
        "                y='é—œéµå­—',\n",
        "                color='æ–¹æ³•',\n",
        "                barmode='group',\n",
        "                title='<b>ä¸åŒåˆ†ææ–¹æ³•çš„é—œéµå­—æ¯”è¼ƒ</b>',\n",
        "                labels={'åˆ†æ•¸': 'åˆ†æ•¸', 'é—œéµå­—': 'é—œéµå­—', 'æ–¹æ³•': 'åˆ†ææ–¹æ³•'},\n",
        "                color_discrete_sequence=px.colors.qualitative.Set2,\n",
        "                height=max(600, len(all_keywords) * 40)\n",
        "            )\n",
        "\n",
        "            fig.update_layout(\n",
        "                template='plotly_white',\n",
        "                xaxis_title='åˆ†æ•¸',\n",
        "                yaxis_title='é—œéµå­—',\n",
        "                legend=dict(\n",
        "                    orientation=\"h\",\n",
        "                    yanchor=\"bottom\",\n",
        "                    y=1.02,\n",
        "                    xanchor=\"right\",\n",
        "                    x=1\n",
        "                ),\n",
        "                margin=dict(l=120, r=50, t=100, b=50)\n",
        "            )\n",
        "\n",
        "            print(f\"âœ… æ–¹æ³•æ¯”è¼ƒåœ–ç”Ÿæˆå®Œæˆ\")\n",
        "            return fig\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ æ–¹æ³•æ¯”è¼ƒåœ–ç”Ÿæˆå¤±æ•—: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def plot_all_methods_heatmap(self, analysis_results, top_n=15):\n",
        "        \"\"\"ç†±åŠ›åœ–\"\"\"\n",
        "        print(\"ğŸ“Š ç”Ÿæˆåˆ†ææ–¹æ³•ç†±åŠ›åœ–\")\n",
        "\n",
        "        try:\n",
        "            methods = {\n",
        "                'word_frequency': 'è©é »çµ±è¨ˆ',\n",
        "                'jieba_tfidf': 'Jieba TF-IDF',\n",
        "                'textrank': 'TextRank',\n",
        "                'sklearn_tfidf': 'Sklearn TF-IDF'\n",
        "            }\n",
        "\n",
        "            all_keywords = set()\n",
        "            for method_key in methods.keys():\n",
        "                if method_key in analysis_results and analysis_results[method_key]:\n",
        "                    for word, _ in analysis_results[method_key][:top_n]:\n",
        "                        all_keywords.add(word)\n",
        "\n",
        "            if not all_keywords:\n",
        "                return None\n",
        "\n",
        "            keywords_list = sorted(list(all_keywords))\n",
        "            matrix = []\n",
        "            method_names = []\n",
        "\n",
        "            for method_key, method_name in methods.items():\n",
        "                if method_key in analysis_results and analysis_results[method_key]:\n",
        "                    method_names.append(method_name)\n",
        "                    keywords_dict = dict(analysis_results[method_key])\n",
        "                    row = [float(keywords_dict.get(kw, 0)) for kw in keywords_list]\n",
        "                    matrix.append(row)\n",
        "\n",
        "            if not matrix:\n",
        "                return None\n",
        "\n",
        "            fig = go.Figure(data=go.Heatmap(\n",
        "                z=matrix,\n",
        "                x=keywords_list,\n",
        "                y=method_names,\n",
        "                colorscale='Viridis',\n",
        "                hovertemplate='æ–¹æ³•: %{y}<br>é—œéµå­—: %{x}<br>åˆ†æ•¸: %{z:.4f}<extra></extra>',\n",
        "                colorbar=dict(title=\"åˆ†æ•¸\")\n",
        "            ))\n",
        "\n",
        "            fig.update_layout(\n",
        "                title='<b>é—œéµå­—åˆ†ææ–¹æ³•ç†±åŠ›åœ–</b>',\n",
        "                xaxis_title='é—œéµå­—',\n",
        "                yaxis_title='åˆ†ææ–¹æ³•',\n",
        "                height=400,\n",
        "                template='plotly_white',\n",
        "                xaxis=dict(tickangle=-45)\n",
        "            )\n",
        "\n",
        "            print(f\"âœ… ç†±åŠ›åœ–ç”Ÿæˆå®Œæˆ\")\n",
        "            return fig\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ ç†±åŠ›åœ–ç”Ÿæˆå¤±æ•—: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def plot_keyword_network(self, keywords, top_n=20):\n",
        "        \"\"\"é—œéµå­—ç¶²çµ¡åœ–\"\"\"\n",
        "        print(\"ğŸ“Š ç”Ÿæˆé—œéµå­—ç¶²çµ¡åœ–\")\n",
        "\n",
        "        try:\n",
        "            top_keywords = keywords[:top_n]\n",
        "\n",
        "            if not top_keywords:\n",
        "                print(\"âš ï¸ æ²’æœ‰é—œéµå­—è³‡æ–™\")\n",
        "                return None\n",
        "\n",
        "            words = [kw for kw, _ in top_keywords]\n",
        "            scores = [float(score) for _, score in top_keywords]\n",
        "\n",
        "            max_score = max(scores)\n",
        "            sizes = [s / max_score * 100 + 20 for s in scores]\n",
        "\n",
        "            import random\n",
        "            random.seed(42)\n",
        "\n",
        "            x_coords = [random.uniform(0, 10) for _ in range(len(words))]\n",
        "            y_coords = [random.uniform(0, 10) for _ in range(len(words))]\n",
        "\n",
        "            fig = go.Figure(data=[go.Scatter(\n",
        "                x=x_coords,\n",
        "                y=y_coords,\n",
        "                mode='markers+text',\n",
        "                marker=dict(\n",
        "                    size=sizes,\n",
        "                    color=scores,\n",
        "                    colorscale='Viridis',\n",
        "                    showscale=True,\n",
        "                    colorbar=dict(title=\"TF-IDF<br>åˆ†æ•¸\"),\n",
        "                    line=dict(width=2, color='white')\n",
        "                ),\n",
        "                text=words,\n",
        "                textposition='middle center',\n",
        "                textfont=dict(size=10, color='white'),\n",
        "                hovertemplate='<b>%{text}</b><br>åˆ†æ•¸: %{marker.color:.4f}<extra></extra>'\n",
        "            )])\n",
        "\n",
        "            fig.update_layout(\n",
        "                title='<b>é—œéµå­—ç¶²çµ¡åœ–ï¼ˆæ³¡æ³¡å¤§å°ä»£è¡¨é‡è¦æ€§ï¼‰</b>',\n",
        "                xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),\n",
        "                yaxis=dict(showgrid=False, showticklabels=False, zeroline=False),\n",
        "                height=700,\n",
        "                template='plotly_white',\n",
        "                showlegend=False\n",
        "            )\n",
        "\n",
        "            print(f\"âœ… ç¶²çµ¡åœ–ç”Ÿæˆå®Œæˆ\")\n",
        "            return fig\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ ç¶²çµ¡åœ–ç”Ÿæˆå¤±æ•—: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def plot_news_timeline(self, news_data):\n",
        "        \"\"\"æ–°èæ™‚é–“è»¸\"\"\"\n",
        "        print(\"ğŸ“Š ç”Ÿæˆæ–°èæ™‚é–“è»¸\")\n",
        "\n",
        "        try:\n",
        "            if not news_data:\n",
        "                print(\"âš ï¸ æ²’æœ‰æ–°èè³‡æ–™\")\n",
        "                return None\n",
        "\n",
        "            df = pd.DataFrame(news_data)\n",
        "            source_counts = df['ä¾†æº'].value_counts()\n",
        "\n",
        "            fig = go.Figure(data=[\n",
        "                go.Bar(\n",
        "                    x=source_counts.index,\n",
        "                    y=source_counts.values,\n",
        "                    marker=dict(\n",
        "                        color=source_counts.values,\n",
        "                        colorscale='Blues',\n",
        "                        showscale=True,\n",
        "                        colorbar=dict(title=\"æ–°èæ•¸é‡\")\n",
        "                    ),\n",
        "                    text=source_counts.values,\n",
        "                    textposition='outside',\n",
        "                    hovertemplate='<b>%{x}</b><br>æ–°èæ•¸é‡: %{y}<extra></extra>'\n",
        "                )\n",
        "            ])\n",
        "\n",
        "            fig.update_layout(\n",
        "                title='<b>æ–°èä¾†æºåˆ†å¸ƒ</b>',\n",
        "                xaxis_title='æ–°èä¾†æº',\n",
        "                yaxis_title='æ–°èæ•¸é‡',\n",
        "                height=400,\n",
        "                template='plotly_white',\n",
        "                showlegend=False\n",
        "            )\n",
        "\n",
        "            print(f\"âœ… æ™‚é–“è»¸ç”Ÿæˆå®Œæˆ\")\n",
        "            return fig\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ æ™‚é–“è»¸ç”Ÿæˆå¤±æ•—: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def create_comprehensive_dashboard(self, analysis_results, news_data):\n",
        "        \"\"\"å»ºç«‹ç¶œåˆå„€è¡¨æ¿\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ğŸ¨ é–‹å§‹ç”Ÿæˆè¦–è¦ºåŒ–å„€è¡¨æ¿\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        dashboard = {}\n",
        "\n",
        "        # 1. æ–‡å­—é›²ï¼ˆå‚³çµ±æ–¹å¼ï¼‰\n",
        "        if 'sklearn_tfidf' in analysis_results and analysis_results['sklearn_tfidf']:\n",
        "            dashboard['wordcloud'] = self.create_wordcloud(\n",
        "                analysis_results['sklearn_tfidf'],\n",
        "                title=\"å‰50ç†±é–€é—œéµå­—æ–‡å­—é›²\",\n",
        "                max_words=50\n",
        "            )\n",
        "\n",
        "        # 2. Plotly æ–‡å­—é›²ï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰\n",
        "        if 'sklearn_tfidf' in analysis_results and analysis_results['sklearn_tfidf']:\n",
        "            dashboard['wordcloud_plotly'] = self.create_wordcloud_plotly(\n",
        "                analysis_results['sklearn_tfidf'],\n",
        "                title=\"é—œéµå­—è¦–è¦ºåŒ–ï¼ˆPlotlyç‰ˆï¼‰\",\n",
        "                max_words=50\n",
        "            )\n",
        "\n",
        "        # 3. é—œéµå­—æ©«æ¢åœ–\n",
        "        if 'sklearn_tfidf' in analysis_results and analysis_results['sklearn_tfidf']:\n",
        "            dashboard['bar_chart'] = self.plot_top_keywords_bar(\n",
        "                analysis_results['sklearn_tfidf'],\n",
        "                title=\"å‰20é—œéµå­—æ’è¡Œæ¦œ\",\n",
        "                top_n=20\n",
        "            )\n",
        "\n",
        "        # 4. åœ“é¤…åœ–\n",
        "        if 'sklearn_tfidf' in analysis_results and analysis_results['sklearn_tfidf']:\n",
        "            dashboard['pie_chart'] = self.plot_keyword_distribution_pie(\n",
        "                analysis_results['sklearn_tfidf'],\n",
        "                title=\"å‰15é—œéµå­—åˆ†æ•¸åˆ†å¸ƒ\",\n",
        "                top_n=15\n",
        "            )\n",
        "\n",
        "        # 5. æ–¹æ³•æ¯”è¼ƒåœ–\n",
        "        dashboard['comparison'] = self.plot_methods_comparison(analysis_results, top_n=10)\n",
        "\n",
        "        # 6. ç†±åŠ›åœ–\n",
        "        dashboard['heatmap'] = self.plot_all_methods_heatmap(analysis_results, top_n=15)\n",
        "\n",
        "        # 7. ç¶²çµ¡åœ–\n",
        "        if 'sklearn_tfidf' in analysis_results and analysis_results['sklearn_tfidf']:\n",
        "            dashboard['network'] = self.plot_keyword_network(\n",
        "                analysis_results['sklearn_tfidf'],\n",
        "                top_n=20\n",
        "            )\n",
        "\n",
        "        # 8. æ–°èæ™‚é–“è»¸\n",
        "        if news_data:\n",
        "            dashboard['timeline'] = self.plot_news_timeline(news_data)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"âœ… è¦–è¦ºåŒ–å„€è¡¨æ¿ç”Ÿæˆå®Œæˆ\")\n",
        "        print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "        return dashboard"
      ],
      "metadata": {
        "id": "gQUuba49lu1y"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "VkExqssCMwU5"
      },
      "outputs": [],
      "source": [
        "# ==================== Google Sheets æ“ä½œ ====================\n",
        "class SheetManager:\n",
        "    \"\"\"Google Sheets ç®¡ç†\"\"\"\n",
        "\n",
        "    def __init__(self, gc, sheet_url):\n",
        "        self.gc = gc\n",
        "        self.spreadsheet = gc.open_by_url(sheet_url)\n",
        "\n",
        "    def write_news_data(self, news_data):\n",
        "        \"\"\"å¯«å…¥æ–°èè³‡æ–™åˆ° Sheet\"\"\"\n",
        "        try:\n",
        "            # å–å¾—æˆ–å»ºç«‹å·¥ä½œè¡¨\n",
        "            try:\n",
        "                sheet = self.spreadsheet.worksheet('æ–°èè³‡æ–™')\n",
        "            except:\n",
        "                sheet = self.spreadsheet.add_worksheet(title='æ–°èè³‡æ–™', rows=1000, cols=10)\n",
        "\n",
        "            # æ¸…ç©ºä¸¦å¯«å…¥æ¨™é¡Œ\n",
        "            sheet.clear()\n",
        "            headers = ['æ¨™é¡Œ', 'æ‘˜è¦', 'é€£çµ', 'ç™¼å¸ƒæ™‚é–“', 'çˆ¬å–æ™‚é–“', 'ä¾†æº']\n",
        "            sheet.append_row(headers)\n",
        "\n",
        "            # å¯«å…¥è³‡æ–™\n",
        "            for news in news_data:\n",
        "                row = [\n",
        "                    news.get('æ¨™é¡Œ', ''),\n",
        "                    news.get('æ‘˜è¦', ''),\n",
        "                    news.get('é€£çµ', ''),\n",
        "                    news.get('ç™¼å¸ƒæ™‚é–“', ''),\n",
        "                    news.get('çˆ¬å–æ™‚é–“', ''),\n",
        "                    news.get('ä¾†æº', '')\n",
        "                ]\n",
        "                sheet.append_row(row)\n",
        "\n",
        "            return f\"âœ… æˆåŠŸå¯«å…¥ {len(news_data)} å‰‡æ–°èåˆ° Google Sheet\"\n",
        "        except Exception as e:\n",
        "            return f\"âŒ å¯«å…¥å¤±æ•—: {str(e)}\"\n",
        "\n",
        "    def read_news_data(self):\n",
        "        \"\"\"å¾ Sheet è®€å–æ–°èè³‡æ–™\"\"\"\n",
        "        try:\n",
        "            sheet = self.spreadsheet.worksheet('æ–°èè³‡æ–™')\n",
        "            data = sheet.get_all_records()\n",
        "            return data\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ è®€å–å¤±æ•—: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def write_keywords(self, keywords):\n",
        "        \"\"\"å¯«å…¥é—œéµå­—çµ±è¨ˆåˆ° Sheet\"\"\"\n",
        "        try:\n",
        "            # å–å¾—æˆ–å»ºç«‹å·¥ä½œè¡¨\n",
        "            try:\n",
        "                sheet = self.spreadsheet.worksheet('é—œéµå­—çµ±è¨ˆ')\n",
        "            except:\n",
        "                sheet = self.spreadsheet.add_worksheet(title='é—œéµå­—çµ±è¨ˆ', rows=100, cols=5)\n",
        "\n",
        "            # æ¸…ç©ºä¸¦å¯«å…¥æ¨™é¡Œ\n",
        "            sheet.clear()\n",
        "            headers = ['æ’å', 'é—œéµå­—', 'TF-IDFåˆ†æ•¸', 'æ›´æ–°æ™‚é–“']\n",
        "            sheet.append_row(headers)\n",
        "\n",
        "            # å¯«å…¥è³‡æ–™\n",
        "            for idx, (keyword, score) in enumerate(keywords, 1):\n",
        "                row = [\n",
        "                    idx,\n",
        "                    keyword,\n",
        "                    round(score, 4),\n",
        "                    datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                ]\n",
        "                sheet.append_row(row)\n",
        "\n",
        "            return f\"âœ… æˆåŠŸå¯«å…¥ {len(keywords)} å€‹é—œéµå­—åˆ°çµ±è¨ˆè¡¨\"\n",
        "        except Exception as e:\n",
        "            return f\"âŒ å¯«å…¥é—œéµå­—å¤±æ•—: {str(e)}\"\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== Gemini AI åˆ†æ ====================\n",
        "def generate_insights_with_gemini(model, news_data, keywords):\n",
        "    \"\"\"ä½¿ç”¨ Gemini ç”Ÿæˆæ´å¯Ÿæ‘˜è¦\"\"\"\n",
        "    try:\n",
        "        # æº–å‚™æç¤ºè©\n",
        "        keywords_text = ', '.join([kw for kw, _ in keywords[:10]])\n",
        "        news_titles = '\\n'.join([f\"- {news['æ¨™é¡Œ']}\" for news in news_data[:20]])\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "ä½ æ˜¯å°ˆæ¥­çš„é‡‘èåˆ†æå¸«ã€‚æ ¹æ“šä»¥ä¸‹å°è‚¡æ–°èè³‡æ–™ï¼Œè«‹æä¾›åˆ†æï¼š\n",
        "\n",
        "ã€æ–°èæ¨™é¡Œã€‘ï¼ˆå…± {len(news_data)} å‰‡ï¼‰\n",
        "{news_titles}\n",
        "\n",
        "ã€é—œéµå­—ã€‘\n",
        "{keywords_text}\n",
        "\n",
        "è«‹æä¾›ï¼š\n",
        "1. **5 å¥é—œéµæ´å¯Ÿ**ï¼ˆæ¯å¥ 30 å­—å…§ï¼Œåˆ†é»æ¢åˆ—ï¼‰\n",
        "2. **ç¸½çµè«–**ï¼ˆ120 å­—ï¼Œæ·±å…¥åˆ†æå¸‚å ´è¶¨å‹¢èˆ‡æŠ•è³‡å»ºè­°ï¼‰\n",
        "\n",
        "æ ¼å¼å¦‚ä¸‹ï¼š\n",
        "## é—œéµæ´å¯Ÿ\n",
        "1. [æ´å¯Ÿ1]\n",
        "2. [æ´å¯Ÿ2]\n",
        "3. [æ´å¯Ÿ3]\n",
        "4. [æ´å¯Ÿ4]\n",
        "5. [æ´å¯Ÿ5]\n",
        "\n",
        "## ç¸½çµ\n",
        "[120å­—ç¸½çµ]\n",
        "\"\"\"\n",
        "\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Gemini åˆ†æå¤±æ•—: {str(e)}\\n\\nè«‹æª¢æŸ¥ API Key æ˜¯å¦æ­£ç¢º\""
      ],
      "metadata": {
        "id": "Ujr5jnUgNFac"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== ä¸»æµç¨‹æ•´åˆï¼ˆåŠ å…¥è¦–è¦ºåŒ–ï¼‰====================\n",
        "def automated_pipeline(sheet_url, gemini_api_key, crawl_pages=3, analysis_method='comprehensive'):\n",
        "    \"\"\"å®Œæ•´è‡ªå‹•åŒ–æµç¨‹ - åŠ å…¥è¦–è¦ºåŒ–\"\"\"\n",
        "\n",
        "    results = {\n",
        "        'status': [],\n",
        "        'news_data': None,\n",
        "        'keywords': None,\n",
        "        'analysis_details': None,\n",
        "        'insights': None,\n",
        "        'visualizations': None\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Step 1: çˆ¬å–æ–°è\n",
        "        results['status'].append(\"ğŸ” æ­¥é©Ÿ1: é–‹å§‹çˆ¬å–è²¡ç¶“æ–°è...\")\n",
        "        crawler = FinanceNewsCrawler()\n",
        "        news_data = crawler.auto_crawl(max_news=min(30, crawl_pages * 10))\n",
        "\n",
        "        if not news_data:\n",
        "            results['status'].append(\"âŒ çˆ¬å–å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£ç·š\")\n",
        "            return results\n",
        "\n",
        "        results['news_data'] = news_data\n",
        "        results['status'].append(f\"âœ… æˆåŠŸçˆ¬å– {len(news_data)} å‰‡æ–°è\")\n",
        "\n",
        "        # Step 2: å¯«å…¥ Google Sheet\n",
        "        results['status'].append(\"ğŸ“ æ­¥é©Ÿ2: å¯«å…¥ Google Sheet...\")\n",
        "        gc = setup_google_sheets()\n",
        "        sheet_manager = SheetManager(gc, sheet_url)\n",
        "        write_result = sheet_manager.write_news_data(news_data)\n",
        "        results['status'].append(write_result)\n",
        "\n",
        "        # Step 3: ä½¿ç”¨çµå·´åˆ†è©é€²è¡Œæ–‡å­—åˆ†æ\n",
        "        results['status'].append(\"ğŸ”¬ æ­¥é©Ÿ3: ä½¿ç”¨çµå·´åˆ†è©é€²è¡Œæ–‡å­—åˆ†æ...\")\n",
        "        analyzer = TextAnalyzer()\n",
        "\n",
        "        # åˆä½µæ¨™é¡Œå’Œæ‘˜è¦\n",
        "        texts = [f\"{n['æ¨™é¡Œ']} {n.get('æ‘˜è¦', '')}\" for n in news_data]\n",
        "\n",
        "        # åŸ·è¡Œç¶œåˆåˆ†æ\n",
        "        if analysis_method == 'comprehensive':\n",
        "            results['status'].append(\"ğŸ“Š åŸ·è¡Œç¶œåˆåˆ†æï¼ˆè©é » + TF-IDF + TextRankï¼‰...\")\n",
        "            analysis_results = analyzer.comprehensive_analysis(texts, top_n=50)  # æå–å‰50å€‹\n",
        "            results['analysis_details'] = analysis_results\n",
        "\n",
        "            # ä½¿ç”¨ TF-IDF ä½œç‚ºä¸»è¦é—œéµå­—\n",
        "            keywords = analysis_results.get('sklearn_tfidf', [])\n",
        "            if not keywords:\n",
        "                keywords = analysis_results.get('jieba_tfidf', [])\n",
        "        else:\n",
        "            keywords = analyzer.get_best_keywords(texts, top_n=50, method=analysis_method)\n",
        "\n",
        "        results['keywords'] = keywords\n",
        "        results['status'].append(f\"âœ… æå–å‰ 50 å€‹é—œéµå­—\")\n",
        "\n",
        "        # Step 4: å›å¯«é—œéµå­—åˆ°çµ±è¨ˆè¡¨\n",
        "        results['status'].append(\"ğŸ“Š æ­¥é©Ÿ4: å›å¯«é—œéµå­—çµ±è¨ˆ...\")\n",
        "        keyword_result = sheet_manager.write_keywords(keywords[:10])  # åªå¯«å…¥å‰10å€‹\n",
        "        results['status'].append(keyword_result)\n",
        "\n",
        "        # Step 4.5: å›å¯«è©³ç´°åˆ†æçµæœ\n",
        "        if analysis_method == 'comprehensive' and results['analysis_details']:\n",
        "            results['status'].append(\"ğŸ“Š æ­¥é©Ÿ4.5: å›å¯«è©³ç´°åˆ†æçµæœ...\")\n",
        "            try:\n",
        "                sheet_manager.write_comprehensive_analysis(results['analysis_details'])\n",
        "                results['status'].append(\"âœ… è©³ç´°åˆ†æçµæœå·²å¯«å…¥\")\n",
        "            except Exception as e:\n",
        "                results['status'].append(f\"âš ï¸ è©³ç´°åˆ†æå¯«å…¥å¤±æ•—: {str(e)}\")\n",
        "\n",
        "        # Step 5: ç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨\n",
        "        results['status'].append(\"ğŸ¨ æ­¥é©Ÿ5: ç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨...\")\n",
        "        visualizer = DataVisualizer()\n",
        "\n",
        "        if analysis_method == 'comprehensive' and results['analysis_details']:\n",
        "            visualizations = visualizer.create_comprehensive_dashboard(\n",
        "                results['analysis_details'],\n",
        "                news_data\n",
        "            )\n",
        "        else:\n",
        "            # ç°¡åŒ–ç‰ˆè¦–è¦ºåŒ–\n",
        "            visualizations = {\n",
        "                'wordcloud': visualizer.create_wordcloud(keywords, max_words=50),\n",
        "                'bar_chart': visualizer.plot_top_keywords_bar(keywords, top_n=20),\n",
        "                'pie_chart': visualizer.plot_keyword_distribution_pie(keywords, top_n=15),\n",
        "                'network': visualizer.plot_keyword_network(keywords, top_n=20),\n",
        "                'timeline': visualizer.plot_news_timeline(news_data)\n",
        "            }\n",
        "\n",
        "        results['visualizations'] = visualizations\n",
        "        results['status'].append(\"âœ… è¦–è¦ºåŒ–åœ–è¡¨ç”Ÿæˆå®Œæˆ\")\n",
        "\n",
        "        # Step 6: ä½¿ç”¨ Gemini ç”Ÿæˆæ´å¯Ÿ\n",
        "        if gemini_api_key and gemini_api_key.strip():\n",
        "            results['status'].append(\"ğŸ¤– æ­¥é©Ÿ6: ä½¿ç”¨ Gemini AI ç”Ÿæˆæ´å¯Ÿ...\")\n",
        "            try:\n",
        "                model = setup_gemini(gemini_api_key)\n",
        "                insights = generate_insights_with_gemini(model, news_data, keywords[:10], results.get('analysis_details'))\n",
        "                results['insights'] = insights\n",
        "                results['status'].append(\"âœ… Gemini åˆ†æå®Œæˆ\")\n",
        "            except Exception as e:\n",
        "                results['insights'] = f\"âŒ Gemini åˆ†æå¤±æ•—: {str(e)}\\nè«‹æª¢æŸ¥ API Key æ˜¯å¦æ­£ç¢º\"\n",
        "                results['status'].append(\"âš ï¸ Gemini åˆ†æå¤±æ•—ï¼ˆå¯èƒ½æ˜¯ API Key éŒ¯èª¤ï¼‰\")\n",
        "        else:\n",
        "            results['insights'] = \"âš ï¸ æœªæä¾› Gemini API Keyï¼Œè·³é AI åˆ†æ\"\n",
        "            results['status'].append(\"âš ï¸ è·³é Gemini åˆ†æï¼ˆæœªæä¾› API Keyï¼‰\")\n",
        "\n",
        "        results['status'].append(\"\\nğŸ‰ æ‰€æœ‰æ­¥é©Ÿå®Œæˆï¼\")\n",
        "\n",
        "    except Exception as e:\n",
        "        results['status'].append(f\"âŒ æµç¨‹éŒ¯èª¤: {str(e)}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "kiCdFg4cNKpn"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== Gradio ä»‹é¢ï¼ˆæ–‡å­—é›²ä¿®æ­£ç‰ˆï¼‰====================\n",
        "def create_gradio_interface():\n",
        "    \"\"\"å»ºç«‹ Gradio ä»‹é¢ - æ–‡å­—é›²ä¿®æ­£ç‰ˆ\"\"\"\n",
        "\n",
        "    def run_pipeline(sheet_url, gemini_key, pages):\n",
        "        \"\"\"åŸ·è¡Œå®Œæ•´æµç¨‹\"\"\"\n",
        "        results = automated_pipeline(sheet_url, gemini_key, int(pages), 'comprehensive')\n",
        "\n",
        "        # ç‹€æ…‹è¨Šæ¯\n",
        "        status_text = '\\n'.join(results['status'])\n",
        "\n",
        "        # é—œéµå­—è¡¨æ ¼\n",
        "        if results['keywords']:\n",
        "            keywords_df = pd.DataFrame(\n",
        "                results['keywords'][:20],\n",
        "                columns=['é—œéµå­—', 'TF-IDFåˆ†æ•¸']\n",
        "            )\n",
        "            keywords_df.index = range(1, len(keywords_df) + 1)\n",
        "            keywords_df['TF-IDFåˆ†æ•¸'] = keywords_df['TF-IDFåˆ†æ•¸'].round(4)\n",
        "        else:\n",
        "            keywords_df = pd.DataFrame()\n",
        "\n",
        "        # æ–°èè¡¨æ ¼\n",
        "        if results['news_data']:\n",
        "            news_df = pd.DataFrame(results['news_data'])\n",
        "            news_df = news_df[['æ¨™é¡Œ', 'ç™¼å¸ƒæ™‚é–“', 'ä¾†æº', 'é€£çµ']]\n",
        "        else:\n",
        "            news_df = pd.DataFrame()\n",
        "\n",
        "        # Gemini æ´å¯Ÿ\n",
        "        insights_text = results['insights'] if results['insights'] else \"å°šæœªç”Ÿæˆåˆ†æ\"\n",
        "\n",
        "        # è¦–è¦ºåŒ–åœ–è¡¨\n",
        "        viz = results.get('visualizations', {})\n",
        "\n",
        "        wordcloud_img = viz.get('wordcloud')  # å‚³çµ±æ–‡å­—é›²ï¼ˆåœ–ç‰‡ï¼‰\n",
        "        wordcloud_plotly = viz.get('wordcloud_plotly')  # Plotlyæ–‡å­—é›²ï¼ˆäº’å‹•åœ–è¡¨ï¼‰\n",
        "        bar_chart = viz.get('bar_chart')\n",
        "        pie_chart = viz.get('pie_chart')\n",
        "        comparison_chart = viz.get('comparison')\n",
        "        heatmap_chart = viz.get('heatmap')\n",
        "        network_chart = viz.get('network')\n",
        "        timeline_chart = viz.get('timeline')\n",
        "\n",
        "        return (\n",
        "            status_text,\n",
        "            keywords_df,\n",
        "            news_df,\n",
        "            insights_text,\n",
        "            wordcloud_img,  # å‚³çµ±æ–‡å­—é›²\n",
        "            wordcloud_plotly,  # Plotlyæ–‡å­—é›²\n",
        "            bar_chart,\n",
        "            pie_chart,\n",
        "            comparison_chart,\n",
        "            heatmap_chart,\n",
        "            network_chart,\n",
        "            timeline_chart\n",
        "        )\n",
        "\n",
        "    # å»ºç«‹ä»‹é¢\n",
        "    with gr.Blocks(title=\"ğŸš€ å°è‚¡è²¡ç¶“æ–°èåˆ†æç³»çµ±\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"\"\"\n",
        "        # ğŸš€ å°è‚¡è²¡ç¶“æ–°èè‡ªå‹•åŒ–åˆ†æç³»çµ±\n",
        "        ### ğŸ“Š çˆ¬èŸ² â†’ Google Sheet â†’ çµå·´åˆ†è© â†’ TF-IDF â†’ Gemini AI â†’ è¦–è¦ºåŒ–å„€è¡¨æ¿\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                sheet_url_input = gr.Textbox(\n",
        "                    label=\"ğŸ“„ Google Sheet ç¶²å€\",\n",
        "                    placeholder=\"https://docs.google.com/spreadsheets/d/YOUR_SHEET_ID/edit\",\n",
        "                    value=\"https://docs.google.com/spreadsheets/d/1UzNgDMKD_WH3uhfQXdrBPM_z8oFHLh77yahMbu5KlHo/edit?usp=sharing\"\n",
        "                )\n",
        "            with gr.Column(scale=2):\n",
        "                gemini_key_input = gr.Textbox(\n",
        "                    label=\"ğŸ”‘ Gemini API Key (é¸å¡«)\",\n",
        "                    placeholder=\"è¼¸å…¥ä½ çš„ Gemini API Key æˆ–ç•™ç©ºè·³é AI åˆ†æ\",\n",
        "                    type=\"password\"\n",
        "                )\n",
        "            with gr.Column(scale=1):\n",
        "                pages_input = gr.Number(\n",
        "                    label=\"ğŸ“„ çˆ¬å–é æ•¸\",\n",
        "                    value=3,\n",
        "                    minimum=1,\n",
        "                    maximum=5\n",
        "                )\n",
        "\n",
        "        run_button = gr.Button(\"ğŸš€ é–‹å§‹åŸ·è¡Œå®Œæ•´æµç¨‹\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        gr.Markdown(\"---\")\n",
        "\n",
        "        with gr.Tabs():\n",
        "            with gr.Tab(\"ğŸ“‹ åŸ·è¡Œç‹€æ…‹\"):\n",
        "                status_output = gr.Textbox(\n",
        "                    label=\"æµç¨‹ç‹€æ…‹\",\n",
        "                    lines=20,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "            with gr.Tab(\"ğŸ”‘ é—œéµå­—åˆ†æ\"):\n",
        "                gr.Markdown(\"### ğŸ“Š å‰ 20 ç†±é–€é—œéµå­— (TF-IDF åˆ†æ)\")\n",
        "                keywords_output = gr.Dataframe(\n",
        "                    label=\"é—œéµå­—æ’è¡Œ\",\n",
        "                    headers=['é—œéµå­—', 'TF-IDFåˆ†æ•¸'],\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "            with gr.Tab(\"ğŸ“° æ–°èåˆ—è¡¨\"):\n",
        "                gr.Markdown(\"### ğŸ“° çˆ¬å–çš„æ–°èè³‡æ–™\")\n",
        "                news_output = gr.Dataframe(\n",
        "                    label=\"æ–°èåˆ—è¡¨\",\n",
        "                    headers=['æ¨™é¡Œ', 'ç™¼å¸ƒæ™‚é–“', 'ä¾†æº', 'é€£çµ'],\n",
        "                    interactive=False,\n",
        "                    wrap=True\n",
        "                )\n",
        "\n",
        "            with gr.Tab(\"ğŸ¤– AI æ´å¯Ÿ\"):\n",
        "                gr.Markdown(\"### ğŸ¤– Gemini AI æ·±åº¦åˆ†æ\")\n",
        "                insights_output = gr.Markdown(value=\"ç­‰å¾…åŸ·è¡Œ...\")\n",
        "\n",
        "            with gr.Tab(\"â˜ï¸ æ–‡å­—é›²ï¼ˆå‚³çµ±ï¼‰\"):\n",
        "                gr.Markdown(\"### â˜ï¸ å‰50ç†±é–€é—œéµå­—æ–‡å­—é›²ï¼ˆWordCloudï¼‰\")\n",
        "                gr.Markdown(\"ä½¿ç”¨ WordCloud å¥—ä»¶ç”Ÿæˆï¼Œå­—é«”å¤§å°ä»£è¡¨é‡è¦æ€§\")\n",
        "                wordcloud_output = gr.Image(label=\"é—œéµå­—æ–‡å­—é›²\", type=\"filepath\")\n",
        "\n",
        "            with gr.Tab(\"â˜ï¸ æ–‡å­—é›²ï¼ˆäº’å‹•ï¼‰\"):\n",
        "                gr.Markdown(\"### â˜ï¸ é—œéµå­—è¦–è¦ºåŒ–ï¼ˆPlotly äº’å‹•ç‰ˆï¼‰\")\n",
        "                gr.Markdown(\"å¯ç¸®æ”¾ã€æ‹–æ›³ã€æ‡¸åœæŸ¥çœ‹åˆ†æ•¸\")\n",
        "                wordcloud_plotly_output = gr.Plot(label=\"Plotly æ–‡å­—é›²\")\n",
        "\n",
        "            with gr.Tab(\"ğŸ“Š æ©«æ¢åœ–\"):\n",
        "                gr.Markdown(\"### ğŸ“Š å‰ 20 é—œéµå­—æ’è¡Œæ¦œ\")\n",
        "                bar_output = gr.Plot(label=\"é—œéµå­—æ©«æ¢åœ–\")\n",
        "\n",
        "            with gr.Tab(\"ğŸ¥§ åœ“é¤…åœ–\"):\n",
        "                gr.Markdown(\"### ğŸ¥§ é—œéµå­—åˆ†æ•¸åˆ†å¸ƒ\")\n",
        "                pie_output = gr.Plot(label=\"é—œéµå­—åœ“é¤…åœ–\")\n",
        "\n",
        "            with gr.Tab(\"ğŸ”€ æ–¹æ³•æ¯”è¼ƒ\"):\n",
        "                gr.Markdown(\"### ğŸ”€ ä¸åŒåˆ†ææ–¹æ³•çš„é—œéµå­—æ¯”è¼ƒ\")\n",
        "                comparison_output = gr.Plot(label=\"åˆ†ææ–¹æ³•æ¯”è¼ƒ\")\n",
        "\n",
        "            with gr.Tab(\"ğŸ”¥ ç†±åŠ›åœ–\"):\n",
        "                gr.Markdown(\"### ğŸ”¥ é—œéµå­—åˆ†æç†±åŠ›åœ–\")\n",
        "                heatmap_output = gr.Plot(label=\"ç†±åŠ›åœ–\")\n",
        "\n",
        "            with gr.Tab(\"ğŸ•¸ï¸ ç¶²çµ¡åœ–\"):\n",
        "                gr.Markdown(\"### ğŸ•¸ï¸ é—œéµå­—ç¶²çµ¡åœ–ï¼ˆæ³¡æ³¡åœ–ï¼‰\")\n",
        "                network_output = gr.Plot(label=\"é—œéµå­—ç¶²çµ¡åœ–\")\n",
        "\n",
        "            with gr.Tab(\"ğŸ“ˆ æ–°èåˆ†å¸ƒ\"):\n",
        "                gr.Markdown(\"### ğŸ“ˆ æ–°èä¾†æºåˆ†å¸ƒçµ±è¨ˆ\")\n",
        "                timeline_output = gr.Plot(label=\"æ–°èä¾†æºçµ±è¨ˆ\")\n",
        "\n",
        "        # ç¶å®šæŒ‰éˆ•\n",
        "        run_button.click(\n",
        "            run_pipeline,\n",
        "            inputs=[sheet_url_input, gemini_key_input, pages_input],\n",
        "            outputs=[\n",
        "                status_output,\n",
        "                keywords_output,\n",
        "                news_output,\n",
        "                insights_output,\n",
        "                wordcloud_output,  # å‚³çµ±æ–‡å­—é›²\n",
        "                wordcloud_plotly_output,  # Plotlyæ–‡å­—é›²\n",
        "                bar_output,\n",
        "                pie_output,\n",
        "                comparison_output,\n",
        "                heatmap_output,\n",
        "                network_output,\n",
        "                timeline_output\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ---\n",
        "        ### ğŸ’¡ ä½¿ç”¨èªªæ˜\n",
        "\n",
        "        ç¾åœ¨æä¾›**å…©ç¨®æ–‡å­—é›²**ï¼š\n",
        "        1. **â˜ï¸ å‚³çµ±æ–‡å­—é›²**: ä½¿ç”¨ WordCloud å¥—ä»¶ï¼Œè¦–è¦ºæ•ˆæœæœ€ä½³\n",
        "        2. â˜ï¸ äº’å‹•æ–‡å­—é›²: ä½¿ç”¨ Plotlyï¼Œå¯ç¸®æ”¾ã€æ‹–æ›³ã€æ‡¸åœ\n",
        "\n",
        "        #### ğŸ¨ è¦–è¦ºåŒ–åœ–è¡¨\n",
        "        - â˜ï¸ **æ–‡å­—é›²ï¼ˆå…©ç¨®ï¼‰**: å‰50å€‹é—œéµå­—è¦–è¦ºåŒ–\n",
        "        - ğŸ“Š **æ©«æ¢åœ–**: å‰20å€‹é—œéµå­—æ’è¡Œ\n",
        "        - ğŸ¥§ **åœ“é¤…åœ–**: å‰15å€‹é—œéµå­—ä½”æ¯”\n",
        "        - ğŸ”€ **æ–¹æ³•æ¯”è¼ƒ**: 4ç¨®åˆ†ææ–¹æ³•å°æ¯”\n",
        "        - ğŸ”¥ **ç†±åŠ›åœ–**: é—œéµå­—Ã—æ–¹æ³•çŸ©é™£\n",
        "        - ğŸ•¸ï¸ **ç¶²çµ¡åœ–**: æ³¡æ³¡å¤§å°=é‡è¦æ€§\n",
        "        - ğŸ“ˆ **æ–°èåˆ†å¸ƒ**: ä¾†æºçµ±è¨ˆ\n",
        "        \"\"\")\n",
        "\n",
        "        return demo"
      ],
      "metadata": {
        "id": "J91VFUZfNPd7"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== å•Ÿå‹•æ‡‰ç”¨ ====================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ğŸš€ æ­£åœ¨å•Ÿå‹•å°è‚¡è²¡ç¶“æ–°èåˆ†æç³»çµ±...\")\n",
        "    print(\"ğŸ“Š åŒ…å«å®Œæ•´è¦–è¦ºåŒ–å„€è¡¨æ¿\")\n",
        "    print(\"=\" * 60)\n",
        "    app = create_gradio_interface()\n",
        "    app.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "goGwth11NSxq",
        "outputId": "14eec79b-9d90-4152-ce2e-07f5e2badbe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ æ­£åœ¨å•Ÿå‹•å°è‚¡è²¡ç¶“æ–°èåˆ†æç³»çµ±...\n",
            "ğŸ“Š åŒ…å«å®Œæ•´è¦–è¦ºåŒ–å„€è¡¨æ¿\n",
            "============================================================\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://4ac96dcf9f5151531a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4ac96dcf9f5151531a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ é–‹å§‹æ™ºæ…§çˆ¬èŸ²...\n",
            "ğŸ“¡ å˜—è©¦ä½¿ç”¨: RSS Feed\n",
            "ğŸ” æ­£åœ¨çˆ¬å–è¯åˆæ–°èç¶² RSS...\n",
            "âœ… æˆåŠŸçˆ¬å– 30 å‰‡æ–°èï¼ˆè¯åˆæ–°èç¶²ï¼‰\n",
            "âœ… æˆåŠŸï¼ä½¿ç”¨ RSS Feed å–å¾— 30 å‰‡æ–°è\n",
            "ğŸ”§ åˆå§‹åŒ–çµå·´åˆ†è©...\n",
            "âœ… çµå·´åˆ†è©åˆå§‹åŒ–å®Œæˆ\n",
            "ğŸ“š åœç”¨è©æ•¸é‡: 136 å€‹\n",
            "ğŸ“– è‡ªè¨‚è©å½™æ•¸é‡: 98 å€‹\n",
            "\n",
            "============================================================\n",
            "ğŸš€ é–‹å§‹ç¶œåˆæ–‡å­—åˆ†æï¼ˆå·²éæ¿¾ç¶²å€å’Œé›œè¨Šï¼‰\n",
            "============================================================\n",
            "\n",
            "ğŸ“Š æ–¹æ³•1: è©é »çµ±è¨ˆ\n",
            "ğŸ” é€²è¡Œè©é »çµ±è¨ˆ...\n",
            "âœ… çµ±è¨ˆäº† 137 å€‹ä¸åŒçš„è©ï¼Œè¿”å›å‰ 50 å€‹\n",
            "\n",
            "ã€è©é »çµ±è¨ˆã€‘å‰ 10 å€‹é—œéµå­—:\n",
            "  1. img (23.0000)\n",
            "  2. src (23.0000)\n",
            "  3. å°ç©é›» (10.0000)\n",
            "  4. è¯æº–æœƒ (3.0000)\n",
            "  5. Fed (3.0000)\n",
            "  6. åŠå°é«” (3.0000)\n",
            "  7. äººå£½æˆ (2.0000)\n",
            "  8. è´ŠåŠ©å¤¥ä¼´ (2.0000)\n",
            "  9. TPVL (2.0000)\n",
            "  10. å¾µå°ç£ (2.0000)\n",
            "\n",
            "ğŸ“Š æ–¹æ³•2: jieba TF-IDF\n",
            "ğŸ” ä½¿ç”¨ jieba.analyse æå–é—œéµå­—...\n",
            "âœ… æå–äº† 9 å€‹æœ‰æ•ˆé—œéµå­—\n",
            "\n",
            "ã€jieba TF-IDFã€‘å‰ 10 å€‹é—œéµå­—:\n",
            "  1. è¯é‚¦éŠ€è¡Œ (0.0373)\n",
            "  2. è¨˜æ†¶é«” (0.0373)\n",
            "  3. æœé‡‘é¾ (0.0373)\n",
            "  4. é™¶æœ±éš± (0.0373)\n",
            "  5. é»ƒç«‹æˆ (0.0373)\n",
            "  6. ä¼ºæœå™¨ (0.0189)\n",
            "  7. å…§æ”¿éƒ¨ (0.0187)\n",
            "  8. åœé›»é™ (0.0187)\n",
            "  9. æŒ‡æ•¸çºŒ (0.0187)\n",
            "\n",
            "ğŸ“Š æ–¹æ³•3: TextRank æ¼”ç®—æ³•\n",
            "ğŸ” ä½¿ç”¨ TextRank æ¼”ç®—æ³•æå–é—œéµå­—...\n",
            "âœ… æå–äº† 7 å€‹æœ‰æ•ˆé—œéµå­—\n",
            "\n",
            "ã€TextRankã€‘å‰ 10 å€‹é—œéµå­—:\n",
            "  1. é™¶æœ±éš± (0.4135)\n",
            "  2. é»ƒç«‹æˆ (0.3885)\n",
            "  3. è¨˜æ†¶é«” (0.3666)\n",
            "  4. æ¦‚å¿µè‚¡ (0.2595)\n",
            "  5. å…§æ”¿éƒ¨ (0.2541)\n",
            "  6. ä¿¡ç¾©å€ (0.2336)\n",
            "  7. æœé‡‘é¾ (0.2318)\n",
            "\n",
            "ğŸ“Š æ–¹æ³•4: sklearn TF-IDF\n",
            "ğŸ” ä½¿ç”¨ TF-IDF åˆ†æé—œéµå­—...\n",
            "âœ… TF-IDF åˆ†æå®Œæˆï¼Œæå– 50 å€‹é—œéµå­—\n",
            "\n",
            "ã€sklearn TF-IDFã€‘å‰ 10 å€‹é—œéµå­—:\n",
            "  1. img (3.6144)\n",
            "  2. src (3.6144)\n",
            "  3. å°ç©é›» (2.4812)\n",
            "  4. fed (1.0356)\n",
            "  5. è¯æº–æœƒ (1.0356)\n",
            "  6. è¦†è“‹ç‡ (1.0000)\n",
            "  7. åŠå°é«” (0.9688)\n",
            "  8. ç±²ç©æ¥µ (0.9033)\n",
            "  9. åŠ æ¬ŠæŒ‡æ•¸ (0.8218)\n",
            "  10. ä¸‰å¤§æ³•äºº (0.7834)\n",
            "\n",
            "============================================================\n",
            "âœ… ç¶œåˆåˆ†æå®Œæˆ\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "ğŸ¨ é–‹å§‹ç”Ÿæˆè¦–è¦ºåŒ–å„€è¡¨æ¿\n",
            "============================================================\n",
            "ğŸ¨ ç”Ÿæˆæ–‡å­—é›²: å‰50ç†±é–€é—œéµå­—æ–‡å­—é›²\n",
            "âŒ æ–‡å­—é›²ç”Ÿæˆå¤±æ•—: cannot open resource\n",
            "ğŸ¨ ç”Ÿæˆ Plotly æ–‡å­—é›²: é—œéµå­—è¦–è¦ºåŒ–ï¼ˆPlotlyç‰ˆï¼‰\n",
            "âœ… Plotly æ–‡å­—é›²ç”Ÿæˆå®Œæˆ\n",
            "ğŸ“Š ç”Ÿæˆé—œéµå­—æ©«æ¢åœ–: å‰20é—œéµå­—æ’è¡Œæ¦œ\n",
            "âœ… æ©«æ¢åœ–ç”Ÿæˆå®Œæˆ (20 å€‹é—œéµå­—)\n",
            "ğŸ“Š ç”Ÿæˆé—œéµå­—åœ“é¤…åœ–: å‰15é—œéµå­—åˆ†æ•¸åˆ†å¸ƒ\n",
            "âœ… åœ“é¤…åœ–ç”Ÿæˆå®Œæˆ\n",
            "ğŸ“Š ç”Ÿæˆåˆ†ææ–¹æ³•æ¯”è¼ƒåœ–\n",
            "âœ… æ–¹æ³•æ¯”è¼ƒåœ–ç”Ÿæˆå®Œæˆ\n",
            "ğŸ“Š ç”Ÿæˆåˆ†ææ–¹æ³•ç†±åŠ›åœ–\n",
            "âœ… ç†±åŠ›åœ–ç”Ÿæˆå®Œæˆ\n",
            "ğŸ“Š ç”Ÿæˆé—œéµå­—ç¶²çµ¡åœ–\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3580048705.py\", line 45, in create_wordcloud\n",
            "    ).generate_from_frequencies(word_freq)\n",
            "      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/wordcloud/wordcloud.py\", line 453, in generate_from_frequencies\n",
            "    self.generate_from_frequencies(dict(frequencies[:2]),\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/wordcloud/wordcloud.py\", line 506, in generate_from_frequencies\n",
            "    font = ImageFont.truetype(self.font_path, font_size)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/PIL/ImageFont.py\", line 880, in truetype\n",
            "    return freetype(font)\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/PIL/ImageFont.py\", line 877, in freetype\n",
            "    return FreeTypeFont(font, size, index, encoding, layout_engine)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/PIL/ImageFont.py\", line 285, in __init__\n",
            "    self.font = core.getfont(\n",
            "                ^^^^^^^^^^^^^\n",
            "OSError: cannot open resource\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ç¶²çµ¡åœ–ç”Ÿæˆå®Œæˆ\n",
            "ğŸ“Š ç”Ÿæˆæ–°èæ™‚é–“è»¸\n",
            "âœ… æ™‚é–“è»¸ç”Ÿæˆå®Œæˆ\n",
            "\n",
            "============================================================\n",
            "âœ… è¦–è¦ºåŒ–å„€è¡¨æ¿ç”Ÿæˆå®Œæˆ\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}